%!TEX root = ../main.tex

\newpage
\section*{Supplementary}

  {\color{red} Tables S1-S5 will be moved to GitHub}

  \renewcommand{\thefigure}{S\arabic{figure}}
  \setcounter{figure}{0}

  \renewcommand{\thetable}{S\arabic{table}}
  \setcounter{table}{0}

  \input{sections/supplementary_table.tex}

  \subsection*{Processing the FMT data}

    \subsubsection*{Data download and pre-processing}
    The main biological dataset used in this study was the collection of 16S rRNA sequencing reads from stool samples (healthy and autistic individuals) for a fecal microbiome transplant study~\cite{Kang2017}.
    The data containing the 16S sequencing reads (V4 region) was downloaded from Qiita~\cite{qiita} (study ID: 10532).
    Only runs 2, 3, and 4 were used for the subsequent analysis as these runs consisted of paired-end sequencing data and run 1 contained single-end data.
    The sample metadata was updated to contain only BMI, sex, height, weight and experimental group.
    This was necessary as two of the network inference algorithms (\ac{mldm} and FlashWeave) required information about environmental heterogeneity.
    However, these environmental correlations were not included in the current analyses.

    \subsubsection*{Processing using the \ac{micone} pipeline}
    The data was then processed using the \ac{micone} pipeline starting at the \ac{sp} step and ending at the \ac{ni} step with the consensus algorithm.
    The configuration files (main.nf and nextflow.config) used to run the \ac{micone} pipeline as well the details of the pipeline execution (dag, report, timeline and trace) are in the "runs/FMT" directory of the data and scripts repository (\href{https://github.com/segrelab/MiCoNE-pipeline-paper}{https://github.com/segrelab/MiCoNE-pipeline-paper})
    % TODO: Store pipeline results
    The results of the pipeline execution for reproducing the analyses in the manuscript are stored on Zenodo[REF] at ...

  \subsection*{Processing the mock data}

    \subsubsection*{Data download and pre-processing}
    The mock datasets, mock4, mock12 and mock16 used for this study, were obtained from mockrobiota~\cite{Bokulich2016}.
    Mock 4 is a mock community composed of 21 bacterial strains represented in equal abundances in two replicate samples, and the same strains represented in uneven abundances in two other replicate samples.
    Mock 12 is composed of 27 bacterial strains containing closely related taxa, the members of which were chosen in part for their well-separated 16S rRNA gene sequences. Some pairs of strains differ by as little as one nucleotide, but all the strains are distinguishable over the sequenced region of the 16S rRNA gene.
    Mock 16 is a mock community composed of even amounts of purified genomic DNA from 49 bacteria and 10 archaea.
    The datasets did not require any preprocessing and could be directly used as input to the pipeline

    \subsubsection*{Processing using the \ac{micone} pipeline}
    The data was processed using the \ac{micone} pipeline starting at the \ac{sp} step and ending at the \ac{op} step with the filtered taxonomic tables as the final output.
    The configuration files (main.nf and nextflow.config) used to run the \ac{micone} pipeline as well the details of the pipeline execution (dag, report, timeline and trace) are in the "runs/mock*" directory of the data and scripts repository (\href{https://github.com/segrelab/MiCoNE-pipeline-paper}{https://github.com/segrelab/MiCoNE-pipeline-paper})
    % TODO: Store pipeline results
    The results of the pipeline execution for reproducing the analyses in the manuscript are stored on Zenodo at ...

% TODO: Update this section and link
    \subsubsection*{Interpretation of Unifrac results in the DC step}
  In order to verify this claim, for each of these methods, we selected the top 1000 representative sequences for each method and recalculated the distance metrics (Figure \ref{fig:figure_s3}).
  We observe that both the weighted and unweighted UniFrac distances are increased, implying that the top representative sequences generated by the different methods are not as similar to each other.
  Therefore, since the weighted UniFrac distances are lower than the unweighted distances, we conclude that the representative sequences in the middle range of the abundance distribution are those that must be the most similar between the methods.

    Open-reference and de novo clustering methods perform the best under the weighted UniFrac metric and the worst (marginally) under the unweighted UniFrac metric.
    This result can be attributed to the large number of low abundance representative sequences that are generated by these methods.
    Deblur performs poorly under weighted Unifrac and although its performance on the mock4 dataset is the best under unweighted UniFrac, its performance on the other datasets is average.
    The Deblur method returns a very small number of representative sequences (2388) and this could account for the reason for the high dissimilarity with the other methods as well as irregular performance on the mock data.

  \subsection*{Synthetic interaction data}

    \subsubsection*{Data generation}
    The synthetic interaction data for the study were generated using two methods.
    The first method, ``seqtime''~\cite{faustSignaturesEcologicalProcesses2018} utilized generalized Lotka-Volterra (gLV) equations to model the microbial community dynamics and made use of the Klemm–Eguı́luz algorithm to generate clique-based interaction networks~\cite{Rottjers2018}.
    We used the seqtime R package to simulate communities with different numbers of species and samples (see Methods for details).
    The second method, ``NorTA'' used the Normal to Anything (NorTA) approach coupled with a given interaction network topology to generate the abundance distribution of the microbial community~\cite{Kurtz2015}.
    We used the spieceasi R package to simulate communities with different abundance distributions and network topologies (see Methods for details).
    The scripts to generate these datasets can be found in the synthetic data and scripts repository (\href{https://github.com/segrelab/MiCoNE-synthetic-data}{https://github.com/segrelab/MiCoNE-synthetic-data})

    \subsubsection*{Processing using the \ac{micone} pipeline}
    The data was processed using the \ac{micone} pipeline using only the \ac{ni} step with the consensus networks as the final output.
    The configuration files (main.nf and nextflow.config) used to run the \ac{micone} pipeline as well the details of the pipeline execution (dag, report, timeline and trace) are in the "runs/norta" and "runs/seqtime" directories of the data and scripts repository (\href{https://github.com/segrelab/MiCoNE-pipeline-paper}{https://github.com/segrelab/MiCoNE-pipeline-paper})
    % TODO: Store pipeline results
    The results of the pipeline execution for reproducing the analyses in the manuscript are stored on Zenodo at ...


  \subsection*{Network metrics}

  In Table~\ref{tab:network_metrics} we show various global network metrics calculated for each tool in the pipeline.
  All the networks that make use of a particular tool are grouped together, and the following average metrics are calculated for each group:
  \begin{enumerate}
    \item The average shortest path length describes the average of all the shortest paths in the graph. No number is reported if the graph is not connected, therefore, the results indicate that none of the networks that make use of \ac{harmonies}, \ac{cozine}, \ac{spring}, \ac{spieceasi} and Pearson are connected.
    \item The average clustering is the average clustering coefficient of the graph. The closer the value is to 1.0, the more densely connected is the graph. We can observe that the networks that use correlation-based methods have the highest values while the direct-association based methods have the lowest.
    \item The number of connected components is the highest for the direct-association based methods and the lowest for the correlation-based methods. In the case of propr, all the networks have only one giant component.
    \item The modularity metric is the modularity over all partitions in a graph calculated using a label propagation algorithm~\cite{cordascoCommunityDetectionSemisynchronous2010}. Positive values imply that there are more edges between vertices of the same type than we would expect by chance, and negative implies that there are less. The networks inferred by \ac{mldm} report very few edges, and skew the average modularity scores. This could also be an artifact of incomplete converge of the \ac{mldm} algorithm for some combinations.
    \item Node connectivity refers to the minimum number of nodes that must be removed from the graph to make it disconnected. We observe that only the networks generated using propr have a high value since most of these networks are connected.
    \item Degree assortativity coefficient measures the similarity of connections in the graph with respect to the node degree. Again we observe that the direct-association based methods have negative degree assortativity, meaning that there are many hubs in these networks. The correlation-based methods have positive values implying that in these networks nodes with similar degrees attach to one another.
  \end{enumerate}
  All the metrics were calculated using the \texttt{networkx} Python package~\cite{hagbergExploringNetworkStructure2008}.

  \subsection*{p-value merging}

  Fisher~\cite{fisher_224a_1948} proposed that for $k$ independent p-values, each generated by $k$ different methods and denoted by $\bar{P}^i$ (notations are same as used in the "Consensus network and p-value merging" subsection of the Methods), the following will hold true for the statistic $\Psi$:
  \begin{equation*}
    \begin{aligned}
      \Psi &= \sum_{i=1}^k -2 \log \left( \bar{P}^i \right) \\
        \Psi &\sim \chi^2_{2k}
    \end{aligned}
  \end{equation*}

  Brown~\cite{brown_400_1975} extended Fisher's method to dependent p-values by using a re-scaled $\chi^2$ distribution:
  \begin{equation*}
    \Psi \sim c \chi^2_{2f}
  \end{equation*}
  where, $f$ is the degrees of freedom and $c$ is the scale factor and are given by:
  \begin{equation*}
    f = \frac{\mathrm{E}[\Psi]^2}{\mathrm{Var}[\Psi]} ~~~\text{and}~~~ c = \frac{\mathrm{Var}[\Psi]}{2\mathrm{E}[\Psi]} = \frac{k}{f}
  \end{equation*}

  Furthermore, Brown showed that $\mathrm{E}[\Psi]$ and $\mathrm{Var}[\Psi]$ can be calculated via a numerical integration:
  \begin{equation*}
    \mathrm{E}[\Psi] = 2k ~~~\text{and}~~~ \mathrm{Var}[\Psi] = 4k + 2\sum_{i<j} \mathrm{Cov}\left( -2\log(\bar{P}^i), -2\log(\bar{P}^j) \right)
  \end{equation*}

  Kost and McDermott~\cite{kost_combining_2002} further fit a third-order polynomial to approximate the covariance
  \begin{equation}
    \mathrm{Cov}\left( -2\log(\bar{P}^i), -2\log(\bar{P}^j) \right) \approx 3.263 \rho_{ij} + 0.710 \rho_{ij}^2 + 0.027 \rho_{ij}^3
    \label{eqn:suppl_covariance-pvalues}
  \end{equation}
  where, $\rho_{ij}$ is the correlation between method $i$ and method $j$

  The final combined p-value~\cite{Poole_Gibbs_Shmulevich_Bernard_Knijnenburg_2016} is then given by:
  \begin{equation}
    \begin{aligned*}
        & \hat{P}_j = 1.0 - \Phi_{2f}\left( \psi / c \right) \\
        \text{where},~ &\psi = -2 \sum_{i=1}^k \log(\bar{P}^i_j) ~~~\text{and}~~~ \Phi_{2f} = \mathrm{CDF}\left( \chi^2_{2f} \right)
    \end{aligned*}
    \label{eqn:suppl_pvalue-combined}
  \end{equation}

  The p-value merging and consensus method in \ac{micone} (refer Methods) uses Equation~\ref{eqn:suppl_covariance-pvalues} to estimate the covariance of the p-values and Equation~\ref{eqn:suppl_pvalue-combined} to merge the p-values (obtained from bootstrapping) from the different correlation methods.
  Note that we do not use Pearson and Spearman methods in the p-value merging step and these algorithms are only used for demonstration and comparison.
  The combined p-values are used to threshold for significance the correlation-based networks during the consensus network step.

  % TODO: Should we mention the MIND database here?
  \subsection*{The JSON network format and network exports}

    The default format \ac{micone} uses for storing the network files is the JSON (JavaScript Object Notation) format.
    The custom JSON schema we have designed is able to store all network related information pertaining to nodes, links and the metadata related to the links and datasets.
    Additionally, \ac{micone} also supports exporting of networks into a variety of other formats such as edge lists, .gml and cytoscape formats.
    Since, we make use of \texttt{networkx}~\cite{hagbergExploringNetworkStructure2008} for the export functionality, networks can be exported to all formats supported by the package.
    However, not all the corresponding metadata will be exported appropriately, since most formats do not support this additional metadata.
    The details of the format and information about importing/exporting it and other network formats can be found in the \ac{micone} documentation.

  \subsection*{Supplementary discussion}

  Additionally, it is worth pointing out some additional more specific conclusions stemming from the individual steps of our analysis.
  The different denoising/clustering methods differ mostly in their identification of sequences that are in low abundances.
  Hence, they do not have much of an impact on the inferred co-occurrence networks when the sequences of low abundance are removed (Figure~\ref{fig:figure_s1}).
  Comparison of inferred and expected reference sequences and their abundances in mock community datasets has allowed us to identify \ac{dada2} as the method which best recapitulates the expected sequence composition.
  For the chimera checking module, we suggest using the remove bimera method since it was developed in conjunction with \ac{dada2} and its performance does not significantly differ from uchime-denovo.
  For the current work we have decided to focus on the tools most widely used at the time of the analysis.
  Some tools which were not as widely used (e.g. dbOTU3~\cite{Olesen2017}) as well as older popular methods like mothur~\cite{Schloss2009} have not been included in the study, but could be added into the pipelines in future updated analyses.

  The choice of taxonomy database was found to be the most important factor in the inference of microbial co-occurrence networks, contributing $65.4\%$ of the total variance.
  The frequent changes in the taxonomy nomenclature coupled with the frequency of updates to the various 16S reference databases create inherent differences \cite{Balvociute2017} in taxonomy hierarchies in these databases.
  Our analysis revealed that no particular reference database performs better than the others across the different mock dataset benchmarks.
  The default reference database in the pipeline is the \ac{gg} reference database along with the ``Naive Bayes'' classifier as the query tool.
  The reason for our choice stems from the popularity of the \ac{gg} database~\cite{parkEvaluation16SRRNA2018} in taxonomic studies, which would enable easy comparison across datasets.
  However, for newer studies we recommend using SILVA database because of its size and taxonomic comprehensiveness~\cite{iiRESCRIPtReproducibleSequence2021} and since \ac{gg} has not been updated since 2013.
  Additionally, a particular database might be more appropriate than the rest based on specific requirements.
  For example, in order to generate a dataset that is compatible with the \ac{mind} platform~\cite{huResourceComparisonIntegration2022} \ac{ncbi} is the most appropriate choice as it guarantees compatibility of taxonomic hierarchy and therefore comparability with other datasets.
  Furthermore, we also enable users to use custom databases~\cite{Ritari2015,iiRESCRIPtReproducibleSequence2021} with the BLAST and Naive Bayes classifiers that are incorporated into the pipeline (from \ac{qiime2}).
  We suggest that that choice of the database should be made based on possible reported or inferred biases in the representation of given biomes in a specific databases~\cite{Balvociute2017,iiRESCRIPtReproducibleSequence2021}, as choosing taxon-specific databases have also been observed to compromise classification~\cite{rmarcelinoUseTaxonspecificReference2020}.

  The \ac{op} step of the pipeline is second in its contribution to total network variance.
  This can be attributed to the large number of nodes that are added to the final networks when the filtering is turned off.
  Additionally, a very large number of nodes also decreases the accuracy of the network inference algorithms for the same sample size~\cite{peschelNetCoMiNetworkConstruction2020} and increases the computational complexity~\cite{tackmannRapidInferenceDirect2019}.
  We observe that filtering out taxa that are present in low abundances in all samples increases the proportion of taxa in common between taxonomy tables generated using different reference databases (Figure~\ref{fig:figure_s5}), providing another reason for filtering.
  We also observe that the reduction in the number of taxa leads to better agreement in the networks inferred through different methods (Figure~\ref{fig:figure_s1}).
  Moreover, filtering is necessary in order to increase the power in tests of significance when the number of taxa is much greater than the number of samples.
