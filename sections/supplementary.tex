%!TEX root = ../main.tex

\newpage
\section*{Supplementary}

  \renewcommand{\thefigure}{S\arabic{figure}}
  \setcounter{figure}{0}

  \renewcommand{\thetable}{S\arabic{table}}
  \setcounter{table}{0}

  \input{sections/supplementary_table.tex}

  \subsection*{Processing the FMT data}

    \subsubsection*{Data download and pre-processing}
    The main dataset used in this study contained stool samples (healthy and autistic individuals) from a fecal microbiome transplant dataset~\cite{Kang2017}.
    The data containing the 16S sequencing reads (V4 region) was downloaded from Qiita~\cite{qiita} (study id: 10532).
    Only runs 2, 3, and 4 were used for the subsequent analysis because these runs were paired-end sequencing data whereas run 1 was single-end.
    The sample metadata was updated so as to contain only bmi, sex, height, weight and experimental group.
    This was necessary as some of the network inference algorithms (like \ac{mldm}) required information about envioronmental heterogeneity.
    However, these results were not included in the current analyses.

    \subsubsection*{Processing using the \ac{micone} pipeline}
    The data was then processed using the \ac{micone} pipeline starting at the \ac{sp} step and ending at the \ac{ni} step with the consensus algorithm.
    The configuration files (main.nf and nextflow.config) used to run the \ac{micone} pipeline as well the details of the pipeline execution (dag, report, timeline and trace) are in the "runs/FMT" directory of the data and scripts repository (\href{https://github.com/segrelab/MiCoNE-pipeline-paper}{https://github.com/segrelab/MiCoNE-pipeline-paper})
    % TODO: Store pipeline results
    The results of the pipeline execution are stored on Zenodo[REF] at ...

  \subsection*{Processing the mock data}

    \subsubsection*{Data download and pre-processing}
    The mock datasets, mock4, mock12 and mock16 used for this study, were obtained from mockrobiota~\cite{Bokulich2016}.
    Mock 4 is a mock community composed of 21 bacterial strains represented in equal abundances in two replicate samples, and the same strains represented in uneven abundances in two replicate samples.
    Mock 12 is composed of 27 bacterial strains containing closely related taxa, the members of which were chosen in part for their well-separated 16S rRNA gene sequences. Some pairs of strains differ by as little as one nucleotide, but all the strains are distinguishable over the sequenced region of the 16S rRNA gene.
    Mock 16 is a mock community composed of even amounts of purified genomic DNA from 49 bacteria and 10 archaea.
    The datasets did not require any preprocessing and could be directly used as input to the pipeline

    \subsubsection*{Processing using the \ac{micone} pipeline}
    The data was processed using the \ac{micone} pipeline starting at the \ac{sp} step and ending at the \ac{op} step with the filtered taxonomic tables as the final output.
    The configuration files (main.nf and nextflow.config) used to run the \ac{micone} pipeline as well the details of the pipeline execution (dag, report, timeline and trace) are in the "runs/mock*" directory of the data and scripts repository (\href{https://github.com/segrelab/MiCoNE-pipeline-paper}{https://github.com/segrelab/MiCoNE-pipeline-paper})
    % TODO: Store pipeline results
    The results of the pipeline execution are stored on Zenodo[REF] at ...

  \subsection*{Synthetic interaction data}

    \subsubsection*{Data generation}
    The synthetic interaction data for the study were generated using two methods.
    The first method, ``seqtime''~\cite{faustSignaturesEcologicalProcesses2018} utilized generalized Lotka-Volterra (gLV) equations to model the microbial community dynamics and made use of the Klemm–Eguı́luz algorithm to generate clique-based interaction networks~\cite{Rottjers2018}.
    We used the seqtime R package to simulate communities with different numbers of species and samples (see Methods for details).
    The second method, ``NorTA'' used the Normal to Anything (NorTA) approach coupled with a given interaction network topology to generate the abundance distribution of the microbial community~\cite{Kurtz2015}.
    We used the spieceasi R package to simulate communities with different abundance distributions and network topologies (see Methods for details).
    The scripts to generate these datasets can be found in the synthetic data and scripts repository (\href{https://github.com/segrelab/MiCoNE-synthetic-data}{https://github.com/segrelab/MiCoNE-synthetic-data})

    \subsubsection*{Processing using the \ac{micone} pipeline}
    The data was processed using the \ac{micone} pipeline using only the \ac{ni} step with the consensus networks as the final output.
    The configuration files (main.nf and nextflow.config) used to run the \ac{micone} pipeline as well the details of the pipeline execution (dag, report, timeline and trace) are in the "runs/norta" and "runs/seqtime" directories of the data and scripts repository (\href{https://github.com/segrelab/MiCoNE-pipeline-paper}{https://github.com/segrelab/MiCoNE-pipeline-paper})
    % TODO: Store pipeline results
    The results of the pipeline execution are stored on Zenodo[REF] at ...

  \subsection*{Effects of the \ac{dc} and \ac{op} steps on network variability}

    % TODO: Elaborate on this
    Figure~\ref{fig:figure_s1} describes the variation in the inferrred networks in the first two principal axes.
    This Figure is the similar to Figure~\ref{fig:figure2}B and further confirms that the variability in the networks decreases upon filtering out the taxonomic entities at low abundance.

    \begin{figure}[H]
      \centering
      \includegraphics[width=1.0\linewidth]{figure_s1.pdf}
    \end{figure}
    \begin{figure}[H]
      \centering
        \caption{
          \textbf{The variation in the inferred networks in the first two principal axes}
          The plots are colored by the tools or parameters used in the \ac{dc} step (A) and the \ac{op} step (B)
        }
      \label{fig:figure_s1}
    \end{figure}
    \FloatBarrier
    \newpage

    % TODO: Write something about TSNE
    \begin{figure}[H]
      \centering
      \includegraphics[width=1.0\linewidth]{figure_s2.pdf}
    \end{figure}
    \begin{figure}[H]
      \centering
        \caption{
          \textbf{The TSNE plot of the networks inferred by various methods}.
        }
      \label{fig:figure_s2}
    \end{figure}
    \FloatBarrier
    \newpage

  % QUESTION: Should the CCA section be only included in the response letter?


  \subsection*{Similarity of high abundance reference sequences}

    Figure~\ref{fig:figureS3} is similar to Figure~\ref{fig:figure3} shows the average UniFrac distance between reference sequences generated by the various methods in the \ac{dc} step.
    In Figure~\ref{fig:figureS3} however, we use only the top 1000 representative sequences (by abundance) for this calculation.
    We observe that both the weighted and unweighted UniFrac distances are increased compared to the figure with the all the representative sequences.
    This shows that the most abundant representative sequences generated by the methods are not similar to each other.

    \begin{figure}[H]
      \centering
      \includegraphics[width=1.0\linewidth]{figure_s3.pdf}
    \end{figure}
    \begin{figure}[H]
      \centering
        \caption{
          \textbf{The UniFrac distance between the 100 most abundant representative sequences generated by various method in the \ac{dc} step}
        }
      \label{fig:figure_s3}
    \end{figure}
    \FloatBarrier
    \newpage

    We find that the two chimera checking methods, uchime and remove bimera, produce similar outputs.
    This can be verified by comparing the output (representative sequence distributions) of the two chimera checking methods for each denoising method (Figure~\ref{fig:figure_s4}).
    We find that with the exception of de novo and open-reference under the unweighted UniFrac metric, all the other methods have low dissimilarity.
    This is especially true for the \ac{dada2} and Deblur methods which are the recommended denoising methods in the \ac{micone} pipeline.
    % TODO: Can we verify this?
    Therefore, we recommended remove bimera as the default chimera method if one is using \ac{dada2} and uchime when one is using Deblur, since these methods were developed for these respective algorithms.

    \begin{figure}[H]
      \centering
      \includegraphics[width=1.0\linewidth]{figure_s4.pdf}
    \end{figure}
    \begin{figure}[H]
      \centering
        \caption{
        }
      \label{fig:figure_s4}
    \end{figure}
    \FloatBarrier
    \newpage

  \subsection*{Mismatches in all assignments at the \ac{TA} step}

    % TODO: Elaborate on this and work on the caption
    Figure~\ref{fig:figure_s5} is similar to Figure~\ref{fig:figure4}B, but instead of the matching only the top 100 taxonomic entities (by abundance), we match all the assignments from one database with those from the other two databases.
    We observe that the percentage of mismatches are indeed higher when considering all the assignments, implying that matching of the taxonomies in the more abundant sequences are more consistent.

    \begin{figure}[H]
      \centering
      \includegraphics[width=1.0\linewidth]{figure_s5.pdf}
    \end{figure}
    \begin{figure}[H]
      \centering
        \caption{
        }
      \label{fig:figure_s5}
    \end{figure}
    \FloatBarrier
    \newpage

  \subsection*{Network metrics}

  In Table~\ref{tab:network_metrics} we show various global network metrics calculated for each tool in the pipeline.
  All the networks that make use of a particular tool are grouped together and the average metrics are calculated for each group.
  All the metrics were calculated using the \texttt{networkx} Python package~\cite{hagbergExploringNetworkStructure2008}.
  \begin{enumerate}
    \item The average shortest path length describes the average of all the shortest paths in the graph. No number is reported if the graph is not connected, thereforce, none of the networks that make use of \ac{harmonies}, \ac{cozine}, \ac{spring}, \ac{spieceasi} and Pearson are connected.
    \item The average clustering is the average clustering coefficient of the graph. The closer the value is to 1.0, the more densely connected is the graph. We can observe that the networks that use correlation-based methods have the highest values while the direct-association based methods have the lowest.
    \item The number of connected components are the highest for the direct-association based methods and the lowest for the correlation-based methods. In the case of propr, all the networks have only one giant component.
    \item Modularity metric is the average modularity over all partitions in a graph calculated using a label propagation algorithm. Positive values imply that there are more edges between vertices of the same type than we would expect by chance, and negative means that there are less. The networks inferred by \ac{mldm} report very few edges, and skew the average modularity scores.
    \item Node connectivity refers to the minimum number of nodes that must be removed from the graph to make it disconnected. We observe that only the networks generated using propr have a high value since most of these networks are connected.
    \item Edge connectivity refers to the minimum number of edges that must be removed from the graph to make it disconnected. These values are similar to node connectivity.
    \item Degree assortativity coefficient measures the similarity of connections in the graph with respect to the node degree. Again we observe that the direct-association based methods have negative degree assortativity, meaning that there are many hubs in these networks. The correlation-based methods have positive values implying that in these networks nodes with similar degrees attach to one another.
  \end{enumerate}

  \begin{table}[]
  \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
  \hline
  DC & CC            & TA                & OP          & NI         & Average shortest path length & Average clustering & No. of connected components & Modularity & Node connectivity & Edge connectivity & Degree assortativity coefficient \\ \hline
  DB & -             & -                 & -           & -          & 2.066                        & 0.265              & 27.088                      & 0.387      & 1.868             & 1.868             & 0.070                            \\
  OR & -             & -                 & -           & -          & 1.904                        & 0.273              & 24.868                      & -17.060    & 1.877             & 1.877             & 0.099                            \\
  D2 & -             & -                 & -           & -          & 2.263                        & 0.267              & 28.228                      & 0.326      & 1.789             & 1.789             & 0.078                            \\
  DN & -             & -                 & -           & -          & 2.065                        & 0.272              & 25.132                      & 0.302      & 1.754             & 1.754             & 0.093                            \\
  CR & -             & -                 & -           & -          & 1.937                        & 0.276              & 24.289                      & -1.865     & 2.079             & 2.079             & 0.097                            \\
  -  & remove bimera & -                 & -           & -          & 2.070                        & 0.272              & 25.968                      & -7.439     & 1.863             & 1.863             & 0.086                            \\
  -  & uchime        & -                 & -           & -          & 2.066                        & 0.269              & 25.874                      & 0.275      & 1.884             & 1.884             & 0.089                            \\
  -  & -             & BLAST(NCBI)       & -           & -          & 2.218                        & 0.262              & 22.784                      & -3.683     & 1.495             & 1.495             & 0.100                            \\
  -  & -             & NaiveBayes(SILVA) & -           & -          & 1.872                        & 0.275              & 30.716                      & 0.415      & 2.674             & 2.674             & 0.074                            \\
  -  & -             & NaiveBayes(GG)    & -           & -          & 2.086                        & 0.274              & 24.263                      & -7.478     & 1.453             & 1.453             & 0.088                            \\
  -  & -             & -                 & Filter(on)  & -          & 1.975                        & 0.252              & 18.493                      & -7.194     & 1.880             & 1.880             & 0.073                            \\
  -  & -             & -                 & Filter(off) & -          & 2.193                        & 0.291              & 34.174                      & 0.431      & 1.867             & 1.867             & 0.104                            \\
  -  & -             & -                 & -           & propr      & 1.650                        & 0.528              & 1.000                       & 0.000      & 10.583            & 10.583            & 0.036                            \\
  -  & -             & -                 & -           & harmonies  &                              & 0.108              & 82.633                      & 0.757      & 0.000             & 0.000             & 0.340                            \\
  -  & -             & -                 & -           & flashweave & 3.878                        & 0.079              & 1.800                       & 0.458      & 0.383             & 0.383             & -0.032                           \\
  -  & -             & -                 & -           & cozine     &                              & 0.090              & 82.167                      & 0.694      & 0.000             & 0.000             & -0.072                           \\
  -  & -             & -                 & -           & spring     &                              & 0.092              & 11.767                      & 0.566      & 0.000             & 0.000             & -0.035                           \\
  -  & -             & -                 & -           & sparcc     & 1.747                        & 0.525              & 1.033                       & 0.009      & 5.500             & 5.500             & -0.011                           \\
  -  & -             & -                 & -           & spieceasi  &                              & 0.076              & 39.383                      & 0.717      & 0.000             & 0.000             & -0.031                           \\
  -  & -             & -                 & -           & pearson    &                              & 0.457              & 23.883                      & 0.551      & 0.000             & 0.000             & 0.478                            \\
  -  & -             & -                 & -           & spearman   & 1.875                        & 0.551              & 1.450                       & 0.044      & 1.117             & 1.117             & 0.170                            \\
  -  & -             & -                 & -           & mldm       & 2.760                        & 0.130              & 2.267                       & -75.648    & 0.433             & 0.433             & -0.028                           \\ \hline
  \end{tabular}
  \caption{
    \textbf{Table of global network metrics for all combinations of networks}.
    The metrics are averaged for each tool in every step of the pipeline, implying that all combinations of networks that use a particular tool are grouped and averaged and their metrics are reported.
  }
  \label{tab:network_metrics}
  \end{table}


  \subsection*{p-value merging}

  Fisher~\cite{fisher_224a_1948} proposed that for $w$ independent p-values, each generated by $w$ different methods and denoted by $\bar{P}^i$, the following will hold true for the statistic $\Psi$:
  \begin{equation*}
    \begin{aligned}
      \Psi &= \sum_{i=1}^w -2 \log \left( \bar{P}^i \right) \\
        \Psi &\sim \chi^2_{2w}
    \end{aligned}
  \end{equation*}

  Brown~\cite{brown_400_1975} extended Fisher's method to dependent p-values by using a re-scaled $\chi^2$ distribution:
  \begin{equation*}
    \Psi \sim c \chi^2_{2f}
  \end{equation*}
  where, $f$ is the degrees of freedom and $c$ is the scale factor and are given by:
  \begin{equation*}
    f = \frac{\mathrm{E}[\Psi]^2}{\mathrm{Var}[\Psi]} ~~~\text{and}~~~ c = \frac{\mathrm{Var}[\Psi]}{2\mathrm{E}[\Psi]} = \frac{w}{f}
  \end{equation*}

  Furthermore, Brown showed that $\mathrm{E}[\Psi]$ and $\mathrm{Var}[\Psi]$ can be calculated via a numerical integration:
  \begin{equation*}
    \mathrm{E}[\Psi] = 2w ~~~\text{and}~~~ \mathrm{Var}[\Psi] = 4w + 2\sum_{i<j} \mathrm{Cov}\left( -2\log(\bar{P}^i), -2\log(\bar{P}^j) \right)
  \end{equation*}

  Kost and McDermott~\cite{kost_combining_2002} further fit a third-order polynomial to approximate the covariance
  \begin{equation}
    \mathrm{Cov}\left( -2\log(\bar{P}^i), -2\log(\bar{P}^j) \right) \approx 3.263 \rho_{ij} + 0.710 \rho_{ij}^2 + 0.027 \rho_{ij}^3
    \label{eqn:covariance-pvalues}
  \end{equation}
  where, $\rho_{ij}$ is the correlation between method $i$ and method $j$

  The final combined p-value~\cite{Poole_Gibbs_Shmulevich_Bernard_Knijnenburg_2016} is then given by:
  \begin{equation}
    \begin{aligned}
        & \hat{P}_j = 1.0 - \Phi_{2f}\left( \psi / c \right) \\
        \text{where},~ &\psi = -2 \sum_{i=1}^w \log(\bar{P}^i_j) ~~~\text{and}~~~ \Phi_{2f} = \mathrm{CDF}\left( \chi^2_{2f} \right)
    \end{aligned}
    \label{eqn:pvalue-combined}
  \end{equation}

  The p-value merging and consensus method in \ac{micone} (refer Documentation) uses Equation~\ref{eqn:covariance-pvalues} to estimate the covariance of the pvalues and Equation~\ref{eqn:pvalue-combined} to merge the p-values (obtained from bootstrapping) from the different correlation methods.
  Note that we do not use Pearson and Spearman methods in the p-value merging step and these algorithms are only used for demonstration and comparison.
  The combined p-values are used to threshold for significance during the consensus network step.

  \subsection*{Performance of alternate consensus network on NorTA data}

    In Figure~\ref{fig:figure_s6} we calculate the consensus network by also taking into account the networks inferred by the Pearson and Spearman methods.
    We see that the consensus networks in this case do not perform as well as those networks that did not incorporate these two algorithms.
    However, we still observe that the consensus network at some parameter values still outperforms the best network inference method (\ac{spieceasi}).

    \begin{figure}[H]
      \centering
      \includegraphics[width=1.0\linewidth]{figure_s6.pdf}
    \end{figure}
    \begin{figure}[H]
      \centering
        \caption{
        }
      \label{fig:figure_s6}
    \end{figure}
    \FloatBarrier
    \newpage


  \subsection*{Performance of network inference algorithms on seqtime data}

    % TODO: Elaborate on this
    In Figure~\ref{fig:figure_s7} we perform the same analysis we did with the NorTA data using the seqtime data.
    We observe that the best performing network inference algorithm is \ac{spieceasi}, but the consensus algorithms, both the scaled-sum and simple voting methods outperform it in terms of precision.
    These results show that the scaled-sum method is much better suited for inferring robust and accurate interactions from count data regardless of the distributions, topologies and number of samples in the data.

    \begin{figure}[H]
      \centering
      \includegraphics[width=1.0\linewidth]{figure_s7.pdf}
    \end{figure}
    \begin{figure}[H]
      \centering
        \caption{
        }
      \label{fig:figure_s7}
    \end{figure}
    \FloatBarrier
    \newpage


  % TODO: Write more about this and link to documentation
  \subsection*{The JSON network format}

    The default format
