%!TEX root = ../main.tex

\section*{Materials and Methods}

  \subsection*{16S rRNA sequencing Datasets}

  \vspace{-5mm}
    The study uses two kinds of 16S rRNA sequencing datasets: biological datasets and mock datasets.
    Biological datasets are collections of sequencing reads obtained from naturally occurring microbial community samples.
    The current analysis used stool samples from a fecal microbiome transplant study of autism~\cite{Kang2017} as biological datasets.
    The study was composed of multiple sequencing runs and those runs that contained paired end reads (runs 2, 3 and 4), were downloaded from Qiita~\cite{qiita} from https://qiita.ucsd.edu/study/description/10532 and used as input sequences for the \ac{micone} pipeline.
    % TODO: How many samples are there for each and average number of reads.
    Sequences from both control (x samples containing xx reads) and autistic (y samples containing yy reads) patients were included in the analyses but the groups were analyzed independently in the pipeline.
    The mock community 16S datasets are sequencing data obtained for artificially assembled collections of species in known proportions.
    The mock datasets used for this study, obtained from mockrobiota~\cite{Bokulich2016}, are labelled mock4, mock12 and mock16.
    The mock4 community is composed of 21 bacterial strains.
    Two replicate samples from mock4 contain all species in equal abundances, and two additional replicate samples contain the same species in unequal abundances.
    The mock12 community is composed of 27 bacterial strains that include closely related taxa with some pairs having only one to two nucleotide difference from another.
    The mock16 community is composed of 49 bacteria and 10 Archea, all represented in equal amount.

  \subsection*{\ac{micone}}

  \vspace{-5mm}
  The flowchart describing the workflow of \ac{micone} (\acl{micone}), our complete 16S data-analysis pipeline, is shown in Figure \ref{fig:figure1}.
  The pipeline integrates many publicly available tools as well as custom R or Python modules and scripts to extract co-occurrence associations from 16S sequence data.
  Each of these tools corresponds to a distinct R or python module that recapitulates the relevant analyses.
  All such individual modules are available as part of the \ac{micone} package.
  The inputs to the pipeline by default are raw untrimmed 16S rRNA sequence reads, but the software can be alternatively configured to use trimmed sequences, \ac{otu} tables and other types of intermediate data.
  The pipeline supports both paired end and single end reads, and additionally supports independently processing multiple runs and merging the OTU tables in the DC step.
  The final output of the pipeline is the inferred network of co-occurrence relationships among the microbes present in the samples.

  The \ac{micone} pipeline provides both a Python API as well as a command-line interface and only uses a single configuration file (nextflow.config) to contain the configuration parameters.
  The configuration file along with the run file (main.nf) lists the inputs, output and the steps to be performed during runtime, along with the parameters to be used (if different from defaults) for the various steps.
  Since the entire pipeline run-through is stored in the form of a text file (the configuration file), subsequent runs are highly reproducible and changes can be easily tracked using version control.
  It uses the nextflow workflow manager~\cite{Tommaso2015} under the hood, making it readily usable on local machines, cluster or cloud with minimal configuration change.
  It also allows for automatic parallelization of all possible processes, both within and across samples.
  The pipeline is designed to be modular: each tool or method is organized into modules which can be easily modified or replaced.
  This modular architecture simplifies the process of adding new tools (refer to modules section in the \ac{micone} documentation).
  % In addition to the Python package, the entire pipeline has been containerized into a Docker~\cite{Merkel1994} image (\hl{dockerhub link}) for easy deployment and setup.
  The main components of the pipeline are detailed in the subsequent sections.

  \subsection*{Sequence Processing (SP)}
  This module deals with processing the raw multiplexed 16S sequence data into demultiplexed, quality-controlled, trimmed sequences.
  It consists of the following processes: demultiplexing and trimming (includes quality control and filtering).
  The trimming process handles the quality control steps such as trimming adapters and trimming low-quality nucleotide stretches from the sequences.
  The parameters and tools in this process are fixed and are not available for user customization.
  The various tools used for the processes were adapted from \ac{qiime2} v2021.8.0~\cite{bolyenReproducibleInteractiveScalable2019}.

  \subsection*{Denoising and Clustering (DC)}
  \vspace{-5mm}
  This module deals with processing the quality-controlled, trimmed 16S sequence data into \ac{otu} or \ac{esv} count tables.
  It consists of the following processes: denoising (or clustering) and chimera checking.
  The denoise/cluster process handles the conversion of the demultiplexed, trimmed sequences into \ac{otu} or \ac{esv} count tables (some methods, like closed reference and open reference clustering, perform clustering and taxonomy assignment in the same step).
  The chimera checking process handles the removal of chimeric sequences created during the \ac{pcr} step.
  The output of this module is a matrix of counts, that describes the number of reads of a particular \ac{otu} or \ac{esv} (rows of the matrix) present in each sample (columns of the matrix).
  The options currently available in the pipeline for denoising and clustering are: open reference clustering, closed reference clustering and de novo clustering methods from the vsearch plugin of \ac{qiime2} v2021.8.0~\cite{bolyenReproducibleInteractiveScalable2019} and denoising methods from \ac{dada2} v1.14~\cite{Callahan2016} and Deblur v1.1.0~\cite{Amir2017} (from the deblur plugin of \ac{qiime2}).
  The quality filtering and chimera checking tools are derived from those used in \ac{qiime2} v2021.8.0 (uchime method) and \ac{dada2} (remove bimera method).

  \begin{table}[h]
    \centering
    \small
    \begin{tabular}{|c|c|c|}
      \hline
      \textbf{Module} & \textbf{Tool} & \textbf{References} \\
      \hline
      Denoising and Clustering & Closed reference & \cite{rognesVSEARCHVersatileOpen2016,bolyenReproducibleInteractiveScalable2019} \\
      Denoising and Clustering & Open reference & \cite{rognesVSEARCHVersatileOpen2016,bolyenReproducibleInteractiveScalable2019} \\
      Denoising and Clustering & De novo & \cite{rognesVSEARCHVersatileOpen2016,bolyenReproducibleInteractiveScalable2019} \\
      Denoising and Clustering & Dada2 & \cite{Callahan2016} \\
      Denoising and Clustering & Deblur & \cite{Amir2017,bolyenReproducibleInteractiveScalable2019} \\
      \hline
      Chimera checking & Uchime & \cite{rognesVSEARCHVersatileOpen2016,bolyenReproducibleInteractiveScalable2019} \\
      Chimera checking & Remove bimera & \cite{Callahan2016} \\
      \hline
    \end{tabular}
    \caption{Tools used in the DC module}
    \label{tab:dc_tools}
  \end{table}

  \subsection*{Taxonomy Assignment (TA)}
  \vspace{-5mm}
  This module deals with assigning taxonomies to either the representative sequences of the \ac{otu}s or directly to the \ac{esv}s.
  In order to assign taxonomies to a particular sequence we need a taxonomy database and a query tool.
  The taxonomy database contains the collection of 16S sequences of micro-organisms of interest and the query tool allows one to compare a sequence of interest to all the sequences in the database to identify the best matches.
  Finally, a consensus method is used to identify the most probable match from the list of best matches.
  The pipeline incorporates \ac{gg} 13\_8~\cite{DeSantis2006}, SILVA 138~\cite{Quast2012} and the \ac{ncbi} (16S RefSeq as of Oct 2021)~\cite{Sayers2009} databases for taxonomy assignment.
  These databases were downloaded and built using the RESCRIPt qiime2 plugin~\cite{iiRESCRIPtReproducibleSequence2021}.
  The Naive Bayes classifier and the \ac{ncbi} blast used as the query tools in this study were from the \ac{qiime2} package and the parameters used were also the defaults in the package.
  The consensus algorithm used is the default method used by the classifiers in \ac{qiime2}.

  \begin{table}[h]
    \centering
    \small
    \begin{tabular}{|c|c|c|}
      \hline
      \textbf{Module} & \textbf{Tool/Database} & \textbf{References} \\
      \hline
      Query tool & Blast & \cite{camachoBLASTArchitectureApplications2009,bokulichOptimizingTaxonomicClassification2018} \\
      Query tool & Naive bayes classifier & \cite{bokulichOptimizingTaxonomicClassification2018} \\
      \hline
      Database & Greengenes & \cite{DeSantis2006} \\
      Database & SILVA & \cite{Quast2012} \\
      Database & NCBI RefSeq & \cite{Sayers2009} \\
      \hline
    \end{tabular}
    \caption{Tools used in the TA module}
    \label{tab:ta_tools}
  \end{table}

  \subsection*{OTU and ESV Processing (OP)}
  \vspace{-5mm}
  This module deals with normalization, filtering, forking, grouping and applying transformations to the \ac{otu} or \ac{esv} counts matrix.
  Normalization of the count matrix involves converting the count matrix of read counts into a count matrix containing relative abundances.
  The module also supports rarefaction, which is a normalization technique used to overcome the bias that might arise due to variable sampling depth in different samples.
  This is performed either by sub-sampling or by normalization of the matrix to the lowest sampling depth~\cite{Weiss2015}.
  Although the pipeline supports rarefaction, these modules are turned off by default and the analyses reported in the paper do not normalize or rareify the counts matrices.
  However, most of the network inference methods perform normalization and other transformation operations on the counts matrix as a part of their workflow.
  Filtering, is performed to remove samples or features (\ac{otu}s or \ac{esv}s) from the count matrix that are sparse.
  By default, samples are filtered out if the total reads in a sample are less than 500 and features are filtered out if the abundance of the 1\% and prevalence is less than 5\%.
  The forking operation splits the count matrix into multiple matrices based on sample metadata column.
  The group operation transforms the \ac{otu} or \ac{esv} count matrix into a taxonomic count matrix at the requested level by adding up counts that map to the same taxonomy.
  Finally, transformations are performed in order to correct for and overcome the compositional bias that is inherent in a counts matrix (in analyses peformed in the study these are handled by the network inference algorithm).
  All the modules in this step were implemented using functions from the \href{http://biom-format.org/}{biom-format} Python package~\cite{mcdonaldBiologicalObservationMatrix2012}.

  \subsection*{Network Inference (NI)}
  \vspace{-5mm}
  This module deals with the inference of co-occurrence associations from the processed \ac{otu} or \ac{esv} counts matrix.
  The input count matrices are collapsed to the Genus level (or any other required taxonomy level) using the group module at the OP step.
  These collapsed matrices are as used as input to the network inference methods to produce association matrices at the appropriate taxonomy level.
  These associations can be represented as a network, with nodes representing taxonomies of the micro-organisms and edges representing the associations between them.
  A null model is created by re-sampling and bootstrapping the counts matrix and recalculating the correlations/associations.
  These bootstrapped association matrices are used to calculate the significance of the inferred associations by calculating the p-values against this null model~\cite{Watts2018}.
  The pipeline includes 4 methods for pairwise correlation metrics, and 6 methods for direct association metrics (refer Table~\ref{tab:ni_tools}).
  The Brown's p-value merging method~\cite{brown_400_1975} is used for combining p-values from the pairwise correlations methods to obtain a consensus p-value, which can be used to filter for significance.
  In the final module of this step, the consensus algorithms are used to create the final consensus network using associations from all the network inference methods.
  The output of this step are co-occurrence association networks encoded in the JSON format (refer Supplementary section).

  \begin{table}[h]
    \centering
    \small
    \begin{tabular}{|c|c|c|}
      \hline
      \textbf{Module} & \textbf{Tool/Database} & \textbf{References} \\
      \hline
      Booststrap & \texttt{fastspar\_bootstraps} v1.0 & \cite{Watts2018} \\
      Booststrap & \texttt{fastspar\_pvalues} v1.0 & \cite{Watts2018} \\
      \hline
      Direct & \ac{spieceasi} v1.1.2 & \cite{Kurtz2015} \\
      Direct & FlashWeave.jl v0.18.1 & \cite{tackmannRapidInferenceDirect2019} \\
      Direct & \ac{cozine} v1.0 & \cite{haCompositionalZeroinflatedNetwork2020a} \\
      Direct & \ac{harmonies} v1.0 & \cite{jiangHARMONIESHybridApproach2020} \\
      Direct & \ac{spring} v1.0.4 & \cite{yoonMicrobialNetworksSPRING2019} \\
      Direct & \ac{mldm} v1.1 & \cite{Yang2017} \\
      \hline
      Correlation & FastSpar (\ac{sparcc}) v1.0 & \cite{Watts2018} \\
      Correlation & Pearson & - \\
      Correlation & Spearman & - \\
      Correlation & propr v2.1.2 & \cite{quinnProprRpackageIdentifying2017} \\
      \hline
    \end{tabular}
    \caption{Tools used in the NI module}
    \label{tab:ni_tools}
  \end{table}


  \subsection*{Consensus Network and p-value merging}
  \vspace{-5mm}

 \subsubsection*{Notation}

 % TODO: Use bold for matrices and lower-case bold for vectors
  This section defines the notation used below to describe the consensus network algorithm used in the our pipeline. Note that all networks to be compared have the same number of nodes.

  $w$ = number of co-occurrence networks to be integrated in the consensus network (corresponding to the number of the different approaches used to infer co-occurrence networks)

  $q$ = number of nodes in any co-occurrence network (assumed to be the same across all $w$ networks)


  $N^i$ = matrix of network edge Weights for the $i^{th}$ co-occurrence network. This is a $q \times q$ matrix, where $i \in \{1,\dots,w\}$. $N^i_{a,b}$ represents edge $(a,b)$ in network $i$

  $P^i$ = matrix of p-values for all edges of the $i^{th}$ co-occurrence network. This is a $q \times q$ matrix, where $i \in \{1,\dots,q\}$


  $\bar{N}^i$ is a "flattened" version of the adjacency matrix $N^i$ into a $q^2 \times 1$ column vector, where all columns are stacked onto each other into a $q^2$ long vector. Element $\bar{N}^i_j$ corresponds to the $j^{th}$ edge in the $i^{th}$ network.

  $\bar{P}^i$ is a "flattened" version of the adjacency matrix $P^i$ into a $q^2 \times 1$ column vector, where all columns are stacked onto each other into a $q^2$ long vector.

  \subsubsection*{Bootstrapping and p-value calculation}
  For all correlation-based methods $k \in \{1,\dots,w\}$, $1000$ permutations of the original \ac{otu} counts data were generated~\cite{Watts2018}.
  All the direct association based methods used in the study have their own regularization methods built-in and hence do not need to undergo bootstrapping.
  We then recalculate the correlations in these permuted \ac{otu} tables using the different correlation-based algorithms.
  Finally, we calculate the p-value based on how often a more extreme association is observed for randomly permuted data.

  \subsubsection*{p-value merging}

  The first part of the consensus algorithm workflow is to merge the p-values for the networks generated by the correlation-based methods.
  This step is performed using the Brown's p-value merging method~\cite{Poole_Gibbs_Shmulevich_Bernard_Knijnenburg_2016,faustCoNetAppInference2016}.


  % TODO: Is this necessary here, should this be elaborated?
  As described in more detail in the Supplementary Material and in the original reference~\cite{Poole_Gibbs_Shmulevich_Bernard_Knijnenburg_2016}, the final combined p-value is given by:
  \begin{equation}
    \begin{aligned}
        & \hat{P}_j = 1.0 - \Phi_{2f}\left( \psi / c \right) \\
        \text{where},~ &\psi = -2 \sum_{i=1}^k \log(\bar{P}^i_j) ~~~\text{and}~~~ \Phi_{2f} = \mathrm{CDF}\left( \chi^2_{2f} \right)
    \end{aligned}
    \label{eqn:pvalue-combined}
  \end{equation}
  where, $\hat{P}_j$ is the combined p-value for the edge $j$, f is the number of degrees of freedom, and c is a scale factor.

  % FIXME: Not sure what our stance on this is?
  Note that we do not use Pearson and Spearman methods in the p-value merging step and these algorithms are only used for demonstration and comparison.
  The combined p-values are used to threshold for significance right before the consensus algorithm is applied to the inferred networks.

  \subsubsection*{Consensus method}
  \vspace{-5mm}

  The consensus algorithm was designed to increase the precision (number of true positives) at the end of the network inference step.
  For this purpose, we designed two simple algorithms that combine the edges reported by the different network inference tools for each pair of nodes using a consensus based on a parameter $p$.
  The inputs to both the algorithms are the co-occurrence networks (association matrices) $\bar{N}^i$ (flattened version of $N^{i}$) generated by each method $i$ and the threshold parameter $p$.
  Here, the $\bar{N}^{i}$ each have the same set of nodes $q$ and only differ by the value of the association inferred between every pair.
  Networks which do not have a particular node, are updated such that the node is added as an isolated component.
  In this manner, $\bar{N}^{i}_j$ represents edge $j$ in network $i$.

    Note that this method is used to filter relevant interactions. However, if a given pair of nodes is inferred to have edges that satisfy the above relationships, all corresponding edges from the $w$ networks will be returned by the algorithm.

  \paragraph*{Algorithm 1: Simple voting}
  % TODO: Add a reference
  The simple voting method performs a voting-based consensus to determine whether an edge will exist between a given node-pair in the final consensus network.
  For each pair of nodes, we determine the number of network inference methods that report an edge $j=k$ between them, i.e. $\bar{N}^{i}_{j=k}, \forall i \in \{1,\dots,w\}$.
  Each node-pair will have an edge in the final consensus network if the number of reported edges is larger than a user-defined threshold.
  (Equation~\ref{eqn:simple-voting2})

 The number of reported edges is computed as follows: for each edge $j$, we obtain $M_j$ which represents the number of networks in which edge $j$ is reported, i.e.:

  \begin{equation}
      M_j = f(g(\bar{N}^{i=1}_j), \dots, g(\bar{N}^{i=w}_j)) \\
    \label{eq:simple-voting}
  \end{equation}

where, $g$ and $f$ are defined as follows:

  \begin{equation*}
    g(x) =
    \begin{cases}
       & 0, \text{ if } x=0, \\
       & -1, \text{ if } x<0, \\
       & 1, \text{ if } x>0
    \end{cases}
  \end{equation*}
  and
  \begin{equation*}
    f(x_1,\dots,x_w) = max \left( \#(i \mid x_i=-1),\#(i \mid x_i=1)
       \right)
  \end{equation*}

 The edge $j$ appears in the final consensus network if the reported edges is greater than the threshold:
  \begin{equation}
     M_j \geq \lfloor p \times w \rfloor
    \label{eqn:simple-voting2}
  \end{equation}
  where,
  $p$ is the user-defined parameter threshold.

  The simple voting method gives the union of the networks when $p \in (0, \frac{1}{w})$ and will return the intersection when $p \in ((w - 1), 1)$.
  In general, if $p \in ((n - 1), \frac{n}{w})$, this algorithm will report an edge in the consensus network when at least $n$ network inference methods report this edge. [REFERENCE]


  \paragraph*{Algorithm 2: Scaled-sum method}

  This algorithm generates a consensus network based on the sum of all edges reported between a pair of nodes. Since in generating this consensus network we combine direct association metrics with correlation-based methods, summing of the edges is preceded by a pre-processing step, in which all networks are re-scaled.

  First, the network generated by each network inference method ($\bar{N}^i$) is re-scaled into a normalized version ($\bar{S}^i$), as follows  (Equation~\ref{eqn:scaled-sum-rescaling}):


  \begin{equation}
    \bar{S}^{i} = \frac{\bar{N}^{i}}{\max(\mid \bar{N}^{i} \mid)}
    \label{eqn:scaled-sum-rescaling}
  \end{equation}

    In this way, it is guaranteed that $\max_{j}(\mid \bar{S}^i_j \mid) = 1$.


  Next, for each edge $(a,b)$, we sum the weights of all reported edges from the different networks (Equation \ref{eqn:scaled-sum}).

  \begin{equation}
    s_j = \sum_i \bar{S}^i_j
    \label{eqn:scaled-sum}
  \end{equation}


  An edge $j$ will be included in the consensus network if:
  \begin{equation}
    \mid s_j \mid > (w - 1) \times p
    \label{eq:scaled-sum2}
  \end{equation}.

  The advantage of this method over the simple voting method is that it also takes into account the strength of the association reported for that particular node in the inferred networks.

  \subsection*{Network Variability}
  \vspace{-5mm}


  \subsubsection*{Notation}
  \vspace{-5mm}
  This section defines notation for the network variability analysis in our study\dots
  % TODO: Update W and Q in the entire subsection
  $W$ =  number of co-occurrence networks generated from all possible combinations of tools and parameters in the workflow. Note that this is different from $w$, which counted only the possible network inference modules.

  $Q$ =  number of combined nodes from all $W$ networks

  % TODO: Should we use different variable names here?
  $N^i$ = The weights of the $i^{th}$ co-occurrence network represented as a $q^\prime \times q^\prime$ adjacency matrix, where $i \in {1, w^\prime}$

  $N^i_{a,b}$ = edge $(a,b)$ in network $i$

  $\bar{N}_i$ is a "flattened" version of the adjacency matrix $N^i$ into a $q^\prime^2 \times 1$ column vector, where all columns are stacked onto each other into a $q^\prime^2$ long vector:


  \subsubsection*{Principal Component Analysis and variability calculation}
  \vspace{-5mm}
 In order to compare across different networks, and analyze the degree of variability induced by the choice of different modules, we organized multiple networks into a single mathematical structure that we could use for linear regression.
 First, we obtain co-occurrence network $\bar{N}_i$ for each of the $W$ possible tool and parameter combinations in the workflow.
 Then, we construct a matrix $\mathbf{\bar{N}}$ whose $i^{th}$ column is the flattened version of the $i^{th}$ network, i.e. the column vector $\bar{N}^i$. Therefore $\bar{N}^i_j$ is the weight of edge $j$ in network $i$. $\bar{N}^i_j$ is assigned a value of 0 if edge $j$ did not exist in network $i$ but was present in other networks. Note that row $j$ of $\mathbf{\bar{N}}$, $\bar{N}_j$ is the vector that encodes the values of edge $j$ across all the networks.

  \begin{equation*}
   \mathbf{\bar{N}} =
     \begin{blockarray}{*{4}{c} l}
      \begin{block}{*{4}{>{$\footnotesize}c<{$}} l}
        $\bar{N}$^1 & $\bar{N}$^2 & \dots & $\bar{N}$^{w^\prime} & \\
      \end{block}
      \begin{block}{[*{4}{c}]>{$\footnotesize}l<{$}}
       n^1_1 & n^2_1 & \cdots  & n^{w^\prime}_1 \bigstrut[t]& $\bar{N}$_1 \\
       n^1_2 & n^2_2 & \cdots  & n^{w^\prime}_2 & $\bar{N}$_2 \\
       \vdots & \vdots & \vdots  & \vdots & \vdots \\
       n^1_{q^\prime^2} & n^2_{q^\prime^2} & \cdots  & n^{w^\prime}_{q^\prime^2} & $\bar{N}$_{q^\prime^2}
      \end{block}
     \end{blockarray}
  \end{equation*}

  To infer the variability contributed due to different steps in the pipeline we perform a linear regression on each edge in $\mathbf{\bar{N}}$ and a subsequent ANOVA to extract the within group variances.
  One issue with this approach is that the possible correlations between the edges of the network could lead to inaccurate estimates of the variance if a linear model were used to directly model the relationships between edges and steps in the workflow.
  In order to remedy this issue, we perform Principal Component Analysis on the matrix $\mathbf{\bar{N}}$ to obtain the $\mathbf{C}$ matrix ($w^\prime \times c$) of components for each network, such that we reduce the dimensions from the $q^\prime^2$ dimensional edge space to a $c$ dimensional component space.

  We then use linear regression to express each component $C_j$ (where $j \in 1:c$) as a linear function of categorical variables that describe the possible options in each of the steps of the pipeline.

  % TODO: Should we remove CC from the LM model?
  In particular, we infer parameters $\alpha_i$ such that:
   \begin{equation*}
       C_j = \sum_{k=1}^5 \left( \alpha^{DC(k)}_j.\delta^{DC(k)}_j \right) +
             \sum_{k=1}^2 \left( \alpha^{CC(k)}_j.\delta^{CC(k)}_j \right) +
             \sum_{k=1}^3 \left( \alpha^{TA(k)}_j.\delta^{TA(k)}_j \right) +
             \sum_{k=1}^2 \left( \alpha^{OP(k)}_j.\delta^{OP(k)}_j \right) +
             \sum_{k=1}^{10} \left( \alpha^{NI(k)}_j.\delta^{NI(k)}_j \right) +
             \epsilon_j
   \end{equation*}

   where, $\alpha_i$ are the coefficients of the regression, $\epsilon_i$ are the residuals and $\delta_i$ are the indicator variables that correspond to the processes utilized in the pipeline used to create the network $N_i$; for example, $\delta^{DC(1)}_i = 1$ if the DC(1) process was used in the generation of the network $N_i$ .
   Here, (i) DC(1) = "closed reference", DC(2) = "open reference", DC(3) = "de novo", DC(4) = "dada2", DC(5) = "deblur";
   (ii) CC(1) = "remove bimera", CC(2) = "uchime";
   (iii)  TA(1) = "GreenGenes", TA(2) = "SILVA", TA(3) = "NCBI";
   (iv) OP(1) = "no filtering", OP(2) = "filtering";
   (v) NI(1) = "SparCC", NI(2) = "Pearson", NI(5) = "Spearman", NI(4) = "Propr", NI(5) = "SpiecEasi", NI(6) = "COZINE", NI(7) = "HARMONIES", NI(8) = "SPRING", NI(9) = "mLDM", NI(10) = "FlashWeave";

  The variance contributed by each step of the pipeline is calculated for every component in $\mathbf{C}$ matrix through ANOVA using the Python \texttt{statsmodels} package and is shown in Figure~\ref{fig:figure2}A.
  The total variance for the network is calculated by adding the variances for each connection.
  The merged network table $\mathbf{\bar{N}}$ is used as the input to the PCA analysis to generate Figure~\ref{fig:figure2}B.

  \subsection*{Synthetic interaction data}
  \vspace{-5mm}

  We generated synthetic interaction data using two methodologies previously used for benchmarking network inference methods.

  The first method, hereby referred to as ``seqtime'', used generalized Lotka-Volterra (gLV) equations to model the microbial community dynamics and utilized the Klemm–Eguı́luz algorithm to generate a clique-based interaction network~\cite{Rottjers2018}.
  We used the \href{https://github.com/hallucigenia-sparsa/seqtime}{seqtime} R package to simulate communities with number of species ($N$) varying from 10 to 150 (10, 25, 50, 100, 150 and 200).
  The initial species concentrations are randomly sampled from a Poisson distribution and the simulation is rerun to generate a number of samples ($S$) varying from 50 to 500 (50, 100, 200, 500) for different communities.
  The abundance values of the species in the community at the end of the simulation time are used to create the OTU table.

  The second method, hereby referred to as ``NorTA'', used the Normal to Anything (NorTA) approach coupled with a given interaction network topology to generate the abundance distribution of the microbial community~\cite{Kurtz2015}.
  We used the \href{https://github.com/zdk123/SpiecEasi}{spieceasi} R package to simulate communities with different network topologies (scale-free, cluster, block, Erdos-Renyi, band and hub) and target abundance distributions (Negative Binomial, Poisson, Zero-Inflated Negative Binomial).
  The OTU table was generated using the American Gut Project example in the \texttt{spieceasi} package (amgut1.filt) with the default parameter options.

  For each method, we generated the OTU table depicting the abundances of species and use this as input to generate association networks using \ac{micone} pipeline.
  The interaction matrix was used as the source of expected (true) interactions and the associations predicted using \ac{micone} were the source of predicted interactions.
  Finally, for each dataset we evaluated the precision and sensitivity of the associations predicted by the individual network inference methods as well as the consensus (Figure \ref{fig:figure7}).

  \subsection*{Code and Data Availability}
  Pipeline: \href{https://github.com/segrelab/MiCoNE}{https://github.com/segrelab/MiCoNE} \\
  Documentation: \href{https://micone.readthedocs.io}{https://micone.readthedocs.io} \\
  Data and scripts: \href{https://github.com/segrelab/MiCoNE-pipeline-paper}{https://github.com/segrelab/MiCoNE-pipeline-paper} \\
  Synthetic data and scripts: \href{https://github.com/segrelab/MiCoNE-synthetic-data}{https://github.com/segrelab/MiCoNE-synthetic-data}
