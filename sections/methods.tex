%!TEX root = ../main.tex

\section*{Materials and Methods}

  \subsection*{Datasets}

  \vspace{-5mm}
  The study uses three kinds of 16S rRNA sequencing datasets: real datasets, mock datasets and synthetic datasets.
  Real datasets are collections of sequencing reads obtained from naturally occurring microbial community samples.
  The current study used healthy stool samples from a fecal microbiome transplant study~\cite{Kang2017} and healthy saliva samples from a periodontal disease study~\cite{Chen2018} as real datasets for analysis.
  % TODO: How many samples are there for each and average number of reads.
  The mock community 16S datasets are real sequencing data obtained for artificially assembled collections of species in known proportions.
  The mock datasets used for this study, obtained from mockrobiota~\cite{Bokulich2016}, are labelled mock4, mock12 and mock16.
  The mock4 community is composed of 21 bacterial strains.
  Two replicate samples from mock4  contain all species in equal abundances, and two additional replicate samples contain the same species in unequal abundances.
  The mock12 community is composed of 27 bacterial strains that include closely related taxa with some pairs having only one to two nucleotide difference from another.
  The mock16 community is composed of 49 bacteria and 10 archea, all represented in equal amount.
  The synthetic datasets were generated using an artificial read simulator called ART~\cite{Huang2012}.
  Three different microbial composition profiles were used as input; reads were generated using a soil and water microbiome composition profiles from the \ac{emp}~\cite{Thompson2017} and healthy gut microbiome project from the fecal microbiome transplant study~\cite{Kang2017}.
  % TODO: Create a section in the supplementary that describes what the original profiles were and how we used ART (what settings) to generate the synthetic data.
  The reads are simulated using the NCBI RefSeq database as the reference sequence pool and the `art\_illumina' sequence profile with a mutation rate of 2\%.
  The scripts used to generate the synthetic data are in the scripts folder of the repository (\href{https://github.com/segrelab/MiCoNE-pipeline-paper}{https://github.com/segrelab/MiCoNE-pipeline-paper}).

  \subsection*{\ac{micone}}

  \vspace{-5mm}
  The flowchart describing the workflow of \ac{micone} (\acl{micone}), our complete 16S data-analysis pipeline, is shown in Figure \ref{fig:figure1}.
  The pipeline integrates many publicly available tools as well as custom R or Python modules and scripts to extract co-occurrence associations from 16S sequence data.
  Each of these tools corresponds to a distinct R or python module that recapitulates the relevant analyses.
  All such individual modules are available as part of the \ac{micone} package.
  The inputs to the pipeline by default are the raw community 16S rRNA sequence reads, but the software can be alternatively configured to use trimmed sequences, \ac{otu} tables and other types of intermediate data.
  The final output of the pipeline is the inferred network of co-occurrence relationships among the microbes present in the samples.

  The \ac{micone} pipeline provides both a Python API as well as a command-line interface and only requires a single configuration file.
  The configuration file lists the inputs, output and the steps to be performed during runtime, along with the parameters to be used (if different from defaults) for the various steps.
  Since the entire pipeline run-through is stored in the form of a text file (the configuration file), subsequent runs are highly reproducible and changes can be easily tracked using version control.
  It uses the nextflow workflow manager~\cite{Tommaso2015} under the hood, making it readily usable on local machines, cluster or cloud with minimal configuration change.
  It also allows for automatic parallelization of all possible processes, both within and across samples.
  The pipeline is designed to be modular: each tool or method is organized into modules which can be easily modified or replaced.
  This modular architecture simplifies the process of adding new tools (refer to modules section in the \ac{micone} documentation).
%   In addition to the Python package, the entire pipeline has been containerized into a Docker~\cite{Merkel1994} image (\hl{dockerhub link}) for easy deployment and setup.
  The main components of the pipeline are detailed in the subsequent sections.

  \subsection*{Denoising and Clustering (DC)}
  \vspace{-5mm}
  This module deals with processing the raw 16S sequence data into \ac{otu} or \ac{esv} count tables.
  It consists of the following processes: quality control, denoising (or clustering) and chimera checking.
  The quality control process handles the demultiplexing and quality control steps such as trimming adapters and trimming low-quality nucleotide stretches from the sequences.
  The denoise/cluster process handles the conversion of the demultiplexed, trimmed sequences into \ac{otu} or \ac{esv} count tables (some methods, like closed reference and open reference clustering, perform clustering and taxonomy assignment in the same step).
  The chimera checking process handles the removal of chimeric sequences created during the \ac{pcr} step.
  The output of this module is a matrix of counts, that describes the number of reads of a particular \ac{otu} or \ac{esv} (rows of the matrix) present in each sample (columns of the matrix).
  The options currently available in the pipeline for denoising and clustering are: open reference clustering, closed reference clustering and de novo clustering methods from \ac{qiime1} v1.9.1~\cite{Caporaso2010} and denoising methods from \ac{dada2} v1.14~\cite{Callahan2016} and Deblur v1.1.0~\cite{Amir2017}.
  The quality filtering and chimera checking tools are derived from those used in \ac{qiime2} v2019.10.0 and \ac{dada2}.


  \subsection*{Taxonomy Assignment (TA)}
  \vspace{-5mm}
  This module deals with assigning taxonomies to either the representative sequences of the \ac{otu}s or directly to the \ac{esv}s.
  In order to assign taxonomies to a particular sequence we need a taxonomy database and a query tool.
  The taxonomy database contains the collection of 16S sequences of micro-organisms of interest and the query tool allows one to compare a sequence of interest to all the sequences in the database to identify the best matches.
  Finally, a consensus method is used to identify the most probable match from the list of best matches.
  The pipeline incorporates \ac{gg} 13\_8~\cite{DeSantis2006}, SILVA 132~\cite{Quast2012} and the \ac{ncbi} (16S RefSeq as of Oct 2019)~\cite{Sayers2009} databases for taxonomy assignment and the Naive Bayes classifier from \ac{qiime2} and \ac{ncbi} blast as the query tools (from \ac{qiime2}).
  The consensus algorithm used is the default method used by the classifiers in \ac{qiime2}.

  % TODO: Add references and basic equations or details
  \subsection*{OTU and ESV Processing (OP)}
  \vspace{-5mm}
  This module deals with normalization, filtering and applying transformations to the \ac{otu} or \ac{esv} counts matrix.
  The module also supports rarefaction, which is a normalization technique used to overcome the bias that might arise due to variable sampling depth in different samples.
  This is performed either by sub-sampling or by normalization of the matrix to the lowest sampling depth~\cite{Weiss2015}.
  Although the pipeline supports rarefaction, the analyses reported in the paper do not rareify the counts matrices.
  Filtering, is performed to remove samples or features (\ac{otu}s or \ac{esv}s) from the count matrix that are sparse.
  In order to determine the filtering threshold we fix the number of samples and correlation detection power needed and determine the number of features to be used.
  Finally, transformations are performed in order to correct for and overcome the compositional bias that is inherent in a counts matrix (in most cases this is handled by the network inference algorithm).

  \subsection*{Network Inference (NI)}
  \vspace{-5mm}
  This module deals with the inference of co-occurrence associations from the \ac{otu} or \ac{esv} counts matrix.
  The counts matrices are collapsed to the Genus level (or the required taxonomy level) by adding up the counts of \ac{otu} or \ac{esv} that map to the same taxa.
  These collapsed matrices are as used as input to the network inferrence methods to produce association matrices at the appropriate taxonomy level.
  These associations can be represented as a network, with nodes representing taxonomies of the micro-organisms and edges representing the associations between them.
  A null model is created by re-sampling and bootstrapping the correlation/interaction matrix and is used to calculate the significance of the inferred associations by calculating the p-values against this null model~\cite{Watts2018}.
  The pipeline includes Pearson, Spearman and FastSpar v0.0.10 (a faster implementation of \ac{sparcc})~\cite{Watts2018} as the pairwise correlation metrics, and \ac{spieceasi} v1.0.7~\cite{Kurtz2015}, \ac{mldm} v1.1~\cite{Yang2017} and \ac{magma}~\cite{Cougoul2019} as the direct association metrics.
  The Brown's p-value merging method~\cite{brown_400_1975} is used for combining p-values from the various methods to obtain a consensus p-value, which is used to create the consensus network.

  \subsection*{Network Variability}
  \vspace{-5mm}
 In order to compare across different networks, and analyze the degree of variability induced by the choice of different modules and parameters, we organized multiple networks into a single mathematical structure that we could use for linear regression.
 In particular, we transformed the adjacency matrix of each co-occurrence network into a vector.
 We then merged the networks generated from all possible combinations of tools into a table (N, see below) in which each column represents one network.

 The merged table $N$ with $p$ edges and $n$ networks, where, each column $N_j$ is the vector representation of one of the networks, each row $L_i$ is the vector representation of one particular edge in all networks (assigned a value of 0 if the edge did not exist in the network but in other networks), and each element $E_{i,j}$ belongs to edge $L_i$ and network $N_j$.

  \begin{equation*}
   N_{p \times n} =
   \begin{bmatrix}
     L_1 \\
     L_2 \\
     \vdots \\
     L_p
   \end{bmatrix}
   =
   \begin{bmatrix}
     E_{1,1} & E_{1,2} & \cdots  & E_{1, n} \\
     E_{2,1} & E_{2,2} & \cdots  & E_{2, n} \\
     \vdots & \vdots & \vdots  & \vdots \\
     E_{p,1} & E_{p,2} & \cdots  & E_{p, n}
   \end{bmatrix}
  \end{equation*}

  In other words, $N$ is the merged table, each column $N_i$ is the vector representation of one of the networks, and each row $L_i$ represents one particular edge in all networks (assigned 0 if the edge does not exist in the network).

  We use linear regression to express each link $L_i$ as a linear function of categorical variables that describe the possible options in each of the first three steps of the pipeline.

  % TODO: Explain the categorical linear model and ANOVA better (text)
  In particular, we infer parameters $\alpha_i$ such that:
   \begin{equation*}
       L_i = \sum_{j=1}^5 \left( \alpha^{DC(j)}_i.\delta^{DC(j)}_i \right) +
             \sum_{j=1}^3 \left( \alpha^{TA(j)}_i.\delta^{TA(j)}_i \right) +
             \sum_{j=1}^2 \left( \alpha^{OP(j)}_i.\delta^{OP(j)}_i \right) +
             \epsilon_i
   \end{equation*}

   where, $\alpha_i$ are the coefficients of the regression, $\epsilon_i$ are the residuals and $\delta_i$ are the indicator variables that correspond to the processes utilized in the pipeline used to create the network $N_i$; for example, $\delta^{DC(1)}_i = 1$ if the DC(1) process was used in the generation of the network $N_i$ .
   Here, (i) DC(1) = "closed reference", DC(2) = "open reference", DC(3) = "de novo", DC(4) = "dada2", DC(5) = "deblur"; (ii)  TA(1) = "GreenGenes", TA(2) = "SILVA", TA(3) = "NCBI"; (iii) OP(1) = "no filtering", OP(2) = "filtering".

  The variance contributed by each step of the pipeline is calculated for every connection in the merged table through ANOVA using the Python statsmodels package and is shown in Figure~\ref{fig:figure2}A.
  The total variance for the network is calculated by adding the variances for each connection.
  The merged network table $N_{p \times n}$ is used as the input to the PCA analysis to generate Figure~\ref{fig:figure2}B.

  \subsection*{Consensus Network and p-value merging}
  \vspace{-5mm}

  \subsubsection*{Bootstrapping}
  For each network inference method (both correlation and direct association based methods), $1000$ permutations of the original \ac{otu} counts data were generated~\cite{Watts2018}.
  We then recalculate the associations in these permuted \ac{otu} tables using the different network inference algorithms.
  Finally, we calculate the p-value based on how often a more extreme association is observed for randomly permuted data.

  \subsubsection*{p-value merging}
  Fisher~\cite{fisher_224a_1948} proposed that for $k$ independent p-values, each generated by $k$ different methods and denoted by $P_i$, the statistic $\Psi$:
  \begin{equation*}
    \begin{aligned}
        \Psi &= \sum_{i=1}^k -2 \log \left( P_i \right) \\
        \Psi &\sim \chi^2_{2k}
    \end{aligned}
  \end{equation*}

  Brown~\cite{brown_400_1975} extended Fisher's method to dependent p-values by using a re-scaled $\chi^2$ distribution:
  \begin{equation*}
    \Psi \sim c \chi^2_{2f}
  \end{equation*}
  where, $f$ is the degrees of freedom and $c$ is the scale factor and are given by:
  \begin{equation*}
    f = \frac{\mathrm{E}[\Psi]^2}{\mathrm{Var}[\Psi]} ~~~\text{and}~~~ c = \frac{\mathrm{Var}[\Psi]}{2\mathrm{E}[\Psi]} = \frac{k}{f}
  \end{equation*}

  Furthermore, Brown showed that $\mathrm{E}[\Psi]$ and $\mathrm{Var}[\Psi]$ can be calculated directly via a numerical integration:
  \begin{equation*}
    \mathrm{E}[\Psi] = 2k ~~~\text{and}~~~ \mathrm{Var}[\Psi] = 4k + 2\sum_{i<j} \mathrm{Cov}\left( -2\log(P_i), -2\log(P_j) \right)
  \end{equation*}

  Kost and McDermott~\cite{kost_combining_2002} further fit a third-order polynomial to approximate the covariance
  \begin{equation}
    \mathrm{Cov}\left( -2\log(P_i), -2\log(P_j) \right) \approx 3.263 \rho_{ij} + 0.710 \rho_{ij}^2 + 0.027 \rho_{ij}^3
    \label{eqn:covariance-pvalues}
  \end{equation}
  where, $\rho_{ij}$ is the correlation between method $i$ and method $j$

  The final combined p-value~\cite{Poole_Gibbs_Shmulevich_Bernard_Knijnenburg_2016} is then given by:
  \begin{equation}
    \begin{aligned}
        & P_{combined} = 1.0 - \Phi_{2f}\left( \psi / c \right) \\
        \text{where},~ &\psi = -2 \sum_{i=1}^k \log(P_i) ~~~\text{and}~~~ \Phi_{2f} = \mathrm{CDF}\left( \chi^2_{2f} \right)
    \end{aligned}
    \label{eqn:pvalue-combined}
  \end{equation}

  The consensus method in \ac{micone} (refer Documentation) uses Equation~\ref{eqn:covariance-pvalues} to estimate the covariance of the pvalues and Equation~\ref{eqn:pvalue-combined} to merge the p-values (obtained from bootstrapping) from different methods.
  Note that we do not use Pearson and Spearman methods in the p-value merging step and these algorithms are only used for demonstration and comparison.
  The combined p-values are used to threshold for significance during the consensus network step.

  \subsection*{Code and Data Availability}
  Pipeline: \href{https://github.com/segrelab/MiCoNE}{https://github.com/segrelab/MiCoNE} \\
  Documentation: \href{https://micone.readthedocs.io}{https://micone.readthedocs.io} \\
  Data and scripts: \href{https://github.com/segrelab/MiCoNE-pipeline-paper}{https://github.com/segrelab/MiCoNE-pipeline-paper}
