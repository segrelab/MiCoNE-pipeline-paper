%!TEX root = ../main.tex

\section*{Materials and Methods}

  \subsection*{Datasets}

  \vspace{-5mm}
  The study uses three kinds of 16S rRNA sequencing datasets \- real datasets, mock datasets and synthetic datasets.
  Real datasets are those sequencing reads obtained from naturally occurring microbial community samples.
  The current study used healthy stool samples from a fecal microbiome transplant study \cite{Kang2017} and healhty saliva samples from a periodontal disease study~\cite{Chen2018} as real datasets for analysis.
  The mock community 16S datasets were obtained from mockrobiota~\cite{Bokulich2016}.
  The mock datasets used for this study were labelled mock4, mock12 and mock16 datasets.
  The synthetic datasets were generated using an artificial read simulator called ART~\cite{Huang2012}.
  Three different microbial composition profiles were used as input
  Synthetic reads were generated using a soil and water microbiome composition profiles from the Earth Microbiome Project~\cite{Thompson2017} and healthy gut microbiome project from the Human Microbiome Project~\cite{HumanMicrobiomeProjectConsortium2012}
  The scripts used to generate the synthetic data are in the \hl{scripts folder of the respository}

  \subsection*{mindpipe}

  \vspace{-5mm}
  The flowchart describing the workflow of the 16S data-analysis pipeline is as shown in Figure \ref{fig:figure1}.
  The pipeline integrates many publicly available tools as well as custom Python modules and scripts to extract co-occurrence associations from 16S sequence data.
  The inputs to the pipeline by default are the raw community 16S rRNA sequence reads, but it can also additionally be configured to use trimmed sequences, \ac{otu} tables and other types of intermediate data.
  The final output of the pipeline is the inferred network of co-occurrence relationships among the microbes present in the samples.

  The mindpipe pipeline provides both a Python API as well as a command-line interface and only requires a single configuration file.
  The configuration file lists the inputs, output and the steps to be performed during runtime, along with the parameters to be used (if different from defaults) for the various steps.
  Since the entire pipeline run-through is stored in the form of a text file (the configuration file), subsequent runs are highly reproducible and changes can be easily tracked using version control.
  It uses the nextflow workflow manager~\cite{Tommaso2015} under the hood and facilitates the usage of the pipeline on the local machine, cluster or cloud with minimal configuration change.
  It also allows for automatic parallelization of all possible processes, both within and across samples.
  The pipeline is designed to be modular, where each tool or method is organized into modules which can be easily modified or replaced.
  This also simplifies the process of adding new tools (refer to modules section in the mindpipe documentation).
  In addition to the Python package, the entire pipeline has been containerized into a Docker~\cite{Merkel1994} image (\hl{dockerhub link}) for easy deployment and setup.
  The main components of the pipeline are detailed in the subsequent sections.

  \subsection*{Denoising and Clustering}
  \vspace{-5mm}
  This module deals with processing the raw 16S sequence data into \ac{otu} or \ac{esv} count tables.
  It consists of three processes:
  \begin{itemize}
    \item The quality control process handles the demultiplexing and quality control steps such as trimming adapters and trimming low-quality nucleotide stretches from the sequences.
    \item The denoise/cluster process handles the conversion of the demultiplexed, trimmed sequences into \ac{otu} or \ac{esv} count tables (some methods perform clustering and taxonomy assignment in the same step).
    \item The chimera checking process handles the removal of chimeric sequences created during the \ac{pcr} step.
  \end{itemize}
  The output of this module is a counts matrix that describes the number of reads of a particular \ac{otu} or \ac{esv} (rows of the matrix) present in each sample (columns of the matrix).
  The pipeline incorporates open reference clustering, closed reference clustering and de novo clustering methods from \ac{qiime1}~\cite{Caporaso2010} and denoising methods from \ac{dada2}~\cite{Callahan2016} and Deblur~\cite{Amir2017}.

  \subsection*{Taxonomy Assignment}
  \vspace{-5mm}
  This module deals with assigning taxonomies to either the representative sequences of the \ac{otu}s or directly to the \ac{esv}s.
  In order to assign taxonomies to a particular sequence we use:
  \begin{itemize}
    \item A taxonomy database, which is a collection of 16S sequences of known organisms.
    \item A query tool, which allows one to compare a sequence of interest to all the sequences in the database to find the best match.
    \item Consensus method
  \end{itemize}
  The pipeline incorporates \ac{gg}~\cite{DeSantis2006}, SILVA~\cite{Quast2012} and the \ac{ncbi}~\cite{Sayers2009} databases for taxonomy assignment and the Naive Bayes classifier from \ac{qiime2} and \ac{ncbi} blast as the query tools.

  % TODO: Add Gabriel's new stuff here
  \subsection*{OTU and ESV Processing}
  \vspace{-5mm}
  This module deals with normalization, filtering and applying transformations on the \ac{otu} or \ac{esv} counts matrix.
  \begin{itemize}
    \item Rarefaction is a normalization technique used to overcome the bias that might arise due to variable sampling depth in different samples. This is performed either by sub-sampling or by normalization of the matrix to the lowest sampling depth.
    \item Filtering is performed to remove samples or features (\ac{otu}s or \ac{esv}s) from the count matrix that are sparse. In order to determine the filtering threshold we fix the number of samples and correlation detection power needed  and determine the number of features to be used.
    \item Transformations are performed in order to correct for and overcome the compositional bias that is inherent in a counts matrix (in most cases this is handled by the network inference algorithm).
  \end{itemize}

  \subsection*{Network Inference}
  \vspace{-5mm}
  This module deals with the inference of co-occurrence associations from the \ac{otu} or \ac{otu} counts matrix.
  These associations can be represented as a network, with nodes representing taxonomies and edges the association between them.
  \begin{itemize}
    \item A null model is created by re-sampling and bootstrapping the correlation/interaction matrix
    \item Significance of these associations is determined by calculating pvalues against this null model.
  \end{itemize}
  The pipeline includes Pearson, Spearman and \ac{sparcc}~\cite{Friedman2012} as the pairwise correlation metrics, and \ac{spieceasi}~\cite{Kurtz2015}, \ac{mldm}~\cite{Yang2017} and \ac{magma}~\cite{} as the direct association metrics.

  \subsection*{Statistical Analysis}

  \textbf{Network Variability}:
  The adjacency matrices of the networks generated from all possible combinations of tools are merged.
  The variance contributed by each step of the pipeline is calculated for every connection in the merged network using ANOVA.
  The total variance for the network is calculated by adding the variances for each connection.
  The PCA analysis is also performed on the merged network.

  \textbf{Power calculations:}

  \textbf{P-value merging}
  The empirical Browns method is used for combining p-values from the various methods to obtain a consensus pvalue, which is used to create the consensus network.

  \subsection*{Code and Data Availability}

  Links here
