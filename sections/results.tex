%!TEX root = ../main.tex

\section*{Results}

  \subsection*{\acl{micone} (\acs{micone})}

  We developed \ac{micone}, a flexible and modular pipeline for the inference of occurrence networks from 16S data.
  It incorporates various popular, publicly available tools as well as custom Python modules for 16S data analysis and network inference (see Methods).
  Using \ac{micone} one can obtain co-occurrence networks by applying to 16S data (or to already processed taxonomic count matrices) any combination of the supported tools.
  The effects of changing any intermediate step can be monitored and evaluated in terms of its final network outcome, as well as on any of the intermediate metrics and data outputs.
  The main workflow of the \ac{micone} pipeline is shown in Figure~\ref{fig:figure1}.
  The different steps that are a part of the co-occurrence network inference workflow can be grouped into five major modules; (i) the sequence processing (SP) step, which handles the demultiplexing, quality control and trimming of the raw 16S sequencing data into demultiplexed and trimmed sequences; (ii) the denoising and clustering (DC) step, which handles denoising of the demultiplexed and trimmed sequences into representative sequences; (iii) the taxonomy assignment (TA) step that assigns taxonomic labels to the representative sequences; (iv) the \ac{otu} processing (OP) step that filters and transforms the taxonomy abundance table; and finally (v) the network inferences (NI) step which infers the microbial co-occurrence network.
  Each process in the pipeline supports alternate tools for performing the same task (see Methods and Figure~\ref{fig:figure1}).
  A centralized configuration file contains all the specifications regarding the modules used in the pipeline, and can be modified by the user to choose the desired set of tools.
  The default tools and parameters for the configuration file can be found in the \ac{micone} documentation (\href{https://micone.readthedocs.io}{https://micone.readthedocs.io}).
  In the current study, we performed a systematic analysis of each step of the pipeline to estimate how much the final co-occurrence network depends on the possible choices at each step.
  We also evaluated numerous tool combinations to determine a set of recommended default options for the pipeline and provide the users with a set of guidelines to facilitate tool selection as appropriate for their data.

  Our analysis involves two types of data: The first type consists of sets of 16S sequencing data from real communities sampled from human stool microbiomes from a fecal microbiome transplant study of autism~\cite{Kang2017}.
  This dataset was chosen because the sequences were easily accessible on Qiita~\cite{qiita} and optimally pre-processed according to the \ac{emp}~\cite{Thompson2017} protocol and can be used directly as input to the \ac{micone} pipeline.
  The second are datasets synthetically or artificially created for the specific goal of helping evaluate computational analysis tools.
  In particular, in order to objectively compare, to the extent possible, how well each step in \ac{micone} best captures the underlying data, we use both mock data (labelled mock4, mock12 and mock16) from mockrobiota~\cite{Bokulich2016} as well as, synthetic networks generated using the NorTA~\cite{Kurtz2015} and seqtime~\cite{Rottjers2018} methods.
  Detailed information on the mock communities and the settings used to generate the synthetic data are provided in the Methods section.

  \FloatBarrier

  \subsection*{DC: Denoising and clustering methods differ in their identification of less common reference sequences}

  The \ac{dc} step is commonly carried out to generate representative sequences from the demultiplexed and trimmed 16S sequencing data to obtain the \ac{otu}/\ac{esv} tables (counts of representative sequences for each sample).
  In order to compare the representative sequence count tables generated by different tools, we processed the same 16S sequencing reads (samples from a fecal microbiome transplant study~\cite{Kang2017}) using 5 different methods: open-reference clustering, closed-reference clustering, de novo clustering, \ac{dada2}~\cite{Callahan2016} and Deblur~\cite{Amir2017}.
  The first three methods are from the vsearch plugin from \ac{qiime2}~\cite{bolyenReproducibleInteractiveScalable2019}.
  The closed and open reference methods in this analysis use the \ac{gg} database for reference sequence alignment.

  To compare the representative sequences generated by these methods we employ both the weighted~\cite{Lozupone2007} (Figure~\ref{fig:figure2}A) and unweighted UniFrac method~\cite{Lozupone2005} (Figure~\ref{fig:figure2}B).
  In both the comparisons, we set a count threshold of 10, such that if the count of the representative sequences in a particular sample is less than the threshold, it is omitted from the analysis.
  The values shown in the figures correspond to the mean UniFrac values across all samples in the study.
  The weighted UniFrac distance metric takes into account the counts of the representative sequences, whereas the unweighted UniFrac distance metric does not and hence gives equal weights to each sequence.
  From Figure~\ref{fig:figure2}A one can see that the representative sequences generated by the different methods, with the exception of Deblur, are similar to each other when weighted by their abundance.
  Figure~\ref{fig:figure2}B on the other hand shows an increase in dissimilarity between each pair of methods suggesting that the methods might differ in the assignment of sequences lower in abundance.
  In order to verify this claim, for each of these methods we selected the top 1000 representative sequences for each method and recalculated the distance metrics (Figure \ref{fig:figure_s3}).
  We observe that the both the weighted and unweighted UniFrac distances are increased implying that the top representative sequences generated by the different methods are not as similar to each other.
  Therefore, since the weighted UniFrac distances are lower than the unweighted distances, we conclude that the representative sequences in the middle range of the abundance distribution are those that must be the most similar between the methods.

  These comparisons only elucidate the similarity or dissimilarity of a pair of methods.
  In order to determine the tool that most accurately recapitulates the reference sequences in the samples, we used the information about the known 16S sequences from the mock datasets (mock4, mock12, and mock16).
  In particular, we used the pipeline to process these datasets using each of the possible methods included for this step.
  Subsequently, we compared the predicted representative sequences with the expected representative sequences and their distribution.
  The results (Figure~\ref{fig:figure2}C and D) show that, the predicted sequence distributions are considerably different from the expected sequence distribution.
  This can be seen in the large values for both unweighted and weighted UniFrac distance values when compared to Figure~\ref{fig:figure2}A and B.
  These high deviations are primarily in one or two of the three datasets that were analyzed and show that the datasets themselves play a big role in the performance of these methods.
  % TODO: How to condense this? Move to supplementary?
  Open-reference and de novo clustering methods perform the best under the weighted UniFrac metric and the worst (marginally) under the unweighted UniFrac metric.
  This result can be attributed to the large number of low abundance representative sequences that are generated by these methods.
  Deblur performs poorly under weighted Unifrac and although its performance on the mock4 dataset is the best under unweighted UniFrac, its performance on the other datasets is average.
  The Deblur method returns a very small number of representative sequences (2388) and this could account for the reason for the high dissimilarity with the other methods as well as irregular performance on the mock data.
  There is no method that outperforms the rest in all datasets.
  Based on their slightly better performance on the mock datasets, their de novo error correcting nature and previous independent evaluation~\cite{Nearing2018}, \ac{dada2} and Deblur appear to be the most reliable.
  This is because the open-reference and de novo clustering methods return a much larger number of \ac{otu}s compared to the other pipelines and this would affect the accuracy of the network inference step if filtering is not performed.
  Overall, since \ac{dada2} as compared to Deblur, displays better performance on all the mock datasets and under the weighted UniFrac metric, we this tool as the default for the DC step of the pipeline.
  However, if comparison across studies that have sequenced different 16S regions is required, closed-reference and open-reference might be a better option.

  After the denoising, the sequences are subject to chimera checking.
  The \ac{micone} pipeline supports two different chimera checking methods, uchime-denovo and remove bimera.
  The ``uchime-denovo'' method is from \ac{qiime2}~\cite{bolyenReproducibleInteractiveScalable2019}, whereas the ``remove bimera'' method is from the \ac{dada2} R package~\cite{Callahan2016}.
  We did not notice any significant difference between the two chimera checking methods (Figure~\ref{fig:figure_s4}), implying that they identify and remove mostly the same set of sequences as chimeras.
  Since the remove bimera method was originally developed in conjunction with dada2 we use this method as the default for the chimera checking module.
  The DC step results in a set of unique sequences, these will be referred to as representative sequences in the subsequent steps.

  \FloatBarrier

  \subsection*{TA: Taxonomy databases vary widely in taxonomy assignments beyond Order level}

  Taxonomy databases are used to assign taxonomic identities to the representative sequences obtained after the DC step.
  The three 16S taxonomic reference databases used in this study are SILVA~\cite{Quast2012}, \ac{gg}~\cite{DeSantis2006} and \ac{ncbi} RefSeq~\cite{Sayers2009} (see Methods).
  These databases vary significantly in terms of taxonomy hierarchies including species names and phylogenetic relationships~\cite{Balvociute2017}.
  Assignment using a particular database also requires a query tool.
  We use the ``Naive Bayes'' classifier from \ac{qiime2} for the \ac{gg} and SILVA databases and the ``BLAST'' tool (also included as a \ac{qiime2} plugin) for the \ac{ncbi} database.
  These tools have been well quantified and optimized~\cite{bokulichOptimizingTaxonomicClassification2018}, hence, we make use of the default parameters in our analyses.

  The representative sequences obtained using the default settings of the DC step were used for taxonomic assignment using the three reference databases.
  Figure~\ref{fig:figure3}A depicts a flow diagram (alluvial plot) that shows how the top 50 representative sequences (sorted by abundance) are assigned a Genus according to the three databases.
  We observe that the top 50 representative sequences are assigned to 17 Genera in \ac{gg}, 23 in SILVA and 17 in \ac{ncbi}, however the sequences that are assigned to a particular Genus vary significantly based on the taxonomy database.
  We observe that not only does the assigned Genus composition vary significantly, but the percentage of unassigned representative sequences (pink) also differ.
  The large percentage of unassigned sequences are because a large fraction of the representative sequences are assigned to an "unknown" Genus during assignment process.
  This is especially evident in two of the three databases, \ac{ncbi} and \ac{gg}.
  A representative sequence might be assigned an "unknown" Genus for one of two reasons: the first is if the taxonomy identifier associated with the sequence in the database did not contain a given Genus; the second, more likely, reason is that the database contains multiple sequences that are very similar to the query (representative) sequence and the consensus algorithm (from \ac{qiime2}) is unable to assign one particular Genus at the required confidence.
  Additionally, we observe that other abundant Genera like Bacteroides and Escherichia are also assigned to different representative sequences, as indicated by the different proportions in the alluvial plot.
  These changes together result in taxonomic assignments generated from each database leading to drastically different taxonomic abundance distributions.

  After assigning all the representative sequences to taxonomies we perform a pairwise comparison of the similarity between the top 100 assignments (by abundance) from different databases at every taxonomic level (Figure~\ref{fig:figure3}B).
  The comparisons of the assignments beyond Family level (Family, Genus and Species) show less than $45\%$ similarity between any pair of databases.
  However, the number of mismatches for taxonomy assignments beyond the Genus level (Genus, Species) are greater than $67\%$, implying that the taxonomy assignments from each reference database are fairly unique and are largely responsible for the differences observed in the co-occurrence networks generated from different taxonomy databases.
  The comparison of all assigned genera (Figure~\ref{fig:figure_s5}), instead of just the top 100, contains a higher percentage of mismatches.
  This suggests that the most abundant sequences are the ones that are consistently matched to the same taxonomies in the different reference databases, at least for the dataset tested in the current analysis.

  As in the previous section, the comparisons so far only indicate similarity or dissimilarity between methods.
  In order to obtain an absolute measure of accuracy of the taxonomic assignments we use the expected reference sequences from the mock datasets as the query sequences for the databases and the expected taxonomic composition as the standard to compare against.
  We use the Bray-Curtis distance metric~\cite{virtanenSciPyFundamentalAlgorithms2020} to calculate the distance between the predicted and expected taxonomic distribution (Figure~\ref{fig:figure3}C).
  Here, we observe that none of the databases perform better than the others in absolute terms and that the dissimilarity with the expected composition is high ($>0.5$ for Family and Genus and $>0.9$ for Species), indicating that all the databases perform poorly when trying to recapture the expected taxonomic composition.
  % TODO: What are the implications?

  Since no database performs better than others against mock datasets, the choice of which database to use could be driven by other reasons.
  One user-specific way to choose, would be based on the known representation of taxa for the microbiome of interest (see Discussion).
  Another reason could be the frequency of updates and the potential for future growth.
  Since the \ac{gg} database is used frequently as a 16S reference database in studies~\cite{Balvociute2017}, to enable easier comparison with published networks in literature, we set \ac{gg} as the default for taxonomy assignment.
  On the other hand, since the degree of similarity of assignments between \ac{ncbi} and \ac{gg} is fairly high (Figure~\ref{fig:figure_s5}), and due to its regular revision and maintenance the \ac{ncbi} database is also a good choice for the user.
  The TA step results in a taxonomic counts table that is used as input to the subsequent steps of the pipeline.
  The count tables at different levels can be obtained through aggregation, for example, Genus count tables were obtained by summing up the counts of the lower taxonomy levels (Species and \ac{otu}) that map to the same higher taxonomy level name.

  \FloatBarrier

  \subsection*{NI: Networks generated using different network inference methods show notable difference in edge-density and connectivity}

  The ten different network inference methods used in this study are: \ac{spieceasi}~\cite{Kurtz2015}, FlashWeave~\cite{tackmannRapidInferenceDirect2019}, \ac{cozine}~\cite{haCompositionalZeroinflatedNetwork2020a}, \ac{harmonies}~\cite{jiangHARMONIESHybridApproach2020}, \ac{spring}~\cite{yoonMicrobialNetworksSPRING2019}, \ac{mldm}~\cite{Yang2017}, \ac{sparcc}~\cite{Friedman2012,Watts2018}, propr~\cite{quinnProprRpackageIdentifying2017}, Spearman and Pearson correlation methods.
  These network inference methods fall into two groups, the first set of methods (Pearson, Spearman, \ac{sparcc}, and propr) infer pairwise correlations while the second set infer direct associations (\ac{spieceasi}, FlashWeave, \ac{cozine}, \ac{harmonies}, \ac{spring}, and \ac{mldm}).
  Pairwise correlation methods involve calculation of the correlation coefficient between each pair of nodes (taxonomic entity like Genera) leading to the inclusion of spurious indirect connections.
  On the other hand, direct association methods use conditional independence to avoid the detection of correlated but indirectly connected taxonomic entities~\cite{Kurtz2015,Menon2018}.

  For the analysis presented in this section, we used the filtered (performed during the \ac{op} step, refer Methods) Genus level counts table obtained using the default settings in the DC and TA steps as the input for the network inference algorithms.
  Figure~\ref{fig:figure4}A shows the networks inferred from this dataset using the different inference algorithms.
  The different networks differ vastly in their edge-density and connectivity; even the edges in common to these networks can have their signs inverted.

  In order to obtain a quantitative picture of the differences between the inferred networks, we analyzed the distribution of common nodes and edges (Figure~\ref{fig:figure4} B and C) using UpSet plots~\cite{Lex}.
  Only \ac{harmonies}, \ac{cozine}, \ac{spieceasi}, \ac{spring}, propr, and \ac{sparcc} are used in the comparison since Pearson and Spearman add a large number of spurious edges as they are not intended for compositional datasets.
  The results for the node intersections show that the networks have $33$ out of $68$ total unique nodes in common and no network possesses an unique node.
  The edge intersections in contrast show that only $8$ edges (out of $202$ total unique edges) are in common between all the methods and each network has a large number of unique edges.
  These results indicate that there is a substantial rewiring of connections in the inferred networks.
  Hence, in order to determine the interactions that are most likely to exist in the dataset we advocate for the development of a consensus algorithm (refer to next section).

  \FloatBarrier

  \subsection*{NI: The consensus network algorithms}

  We observe a high degree of rewiring in the inferred association networks generated by the different methods (Figure \ref{fig:figure4}C), implying that there is a lack of agreement on the associations that should be a part of the final network.
  Therefore, we have developed a consensus method that would take into consideration the evidence offered by each network inference algorithm and generate a consensus network that would contain the most probable interactions for the input data.
  We have developed two algorithms for the generation of the consensus network, the simple voting and the scaled-sum methods (see Methods).
  Both approaches combine network inferred from both correlation-based and direct association methods.
  First, for the correlation-based methods we calculate p-values using null models and then merge the p-values using the Brown's p-value merging method~\cite{Poole_Gibbs_Shmulevich_Bernard_Knijnenburg_2016,faustCoNetAppInference2016} (see Methods).
  Second, we filter all the inferred networks based on an association strength threshold of 0.1 and a p-value cutoff of 0.05.
  Finally, we apply the consensus algorithms we have developed on these filtered networks.
  % The simple voting method does a consensus based on whether the number of inferred networks that have an edge between a pair of nodes is greater than a threshold.
  % The scaled-sum method does a consensus based on whether the normalized sum of an edge between a pair of nodes from all inferred networks is greater than a threshold.
  The default algorithm for the pipeline is the scaled-sum method, since this method takes into account the weights of the associations in the determination of the final consensus.
  By default, of the correlation-based methods only \ac{sparcc} and propr are included in the calculation of consensus network, for consensus networks that include Spearman and Pearson see Figure~\ref{fig:figure_s6} in the Supplementary section.
  Additionally, the pipeline also allows for a different subset of methods to be used in the calculation of the consensus if it is required.
  Based on this approach, \ac{micone} reports as default output, the consensus network, where is each edge is annotated with weights (correlations for the correlation-based methods and direct associations for the other methods) from all the methods used in the consensus algorithm.

  Similar to what was done for the previous steps of the pipeline, we evaluated the network inference algorithms and the final consensus network using synthetic interaction data.
  For this purpose, we generated synthetic interaction data using the ``NorTA''~\cite{Kurtz2015} and ``seqtime''~\cite{faustSignaturesEcologicalProcesses2018} methods (see Methods), which have been previously used for benchmarking network inference methods~\cite{Kurtz2015,Weiss2016}.
  For each method, an \ac{otu} counts table is generated based on the selected interaction network topology and properties.
  This counts table is used as the input to the \ac{micone} pipeline to generate predicted associations.
  The interaction network used to generate the counts table is used as the source of true interactions to calculate the precision and sensitivity for each network inference algorithm for the NorTA datasets (Figure \ref{fig:figure5}).
  The goal of the consensus network algorithms is to increase the precision of the resultant consensus network by removing edges that are either not present in most of the inference methods or whose association strength is low.
  In Figure \ref{fig:figure5} we show that the consensus algorithm, especially the scaled-sum method, captures true associations with more precision, but as a consequence the sensitivity of the predictions is reduced.
  Overall, we see that the scaled-sum method for $p=1.0$ performs the best in terms of average precision ($1.000$).
  However, the scaled-sum method for $p=0.667$ has the second-highest average precision ($0.985$) and does not underperform in terms of sensitivity.
  Among the individual inference methods, we observe that \ac{spieceasi} has the best average precision ($0.944$)
  Therefore, the consensus networks provide the means to obtain a short list of associations that would have a high probability of being present in the real interaction network.
  Furthermore, we also find that the consensus algorithm performs drastically better in terms of precision (0.820 for scaled-sum method vs. 0.624 for \ac{spieceasi}) in recapturing the original interactions in the ``seqtime'' data.
  The precision and sensitivity graphs on the seqtime data can be found in the Supplementary section (Figure~\ref{fig:figure_s7}).

  \FloatBarrier

  \subsection*{Overall assessment of impact on inferred networks}

  In order to analyze the effect of different processing methods on the inferred co-occurrence networks, we generated these networks using all possible combinations of methods and estimated the variability in the networks due to each choice (Figure \ref{fig:figure6}A).
  The networks used in this analysis are those directly inferred from the correlation-based and direct inference methods and to which the consensus algorithm is yet to be applied.
  The effects of various steps on the final co-occurrence network are estimated by building a linear model of the edges of the network as a function the various steps in the pipeline workflow (see Methods).
  Figure \ref{fig:figure6}A, shows the percentage of total variation among the co-occurrence networks due to the different steps of the pipeline.
  The \ac{ta} step or more specifically the choice of 16S reference database, contributes the most ($\sim65.4\%$) to the variation in the networks, followed by the \ac{op} step ($\sim26.8\%$).
  This result highlights the importance of the taxonomy assignment step in the 16S data analysis workflow, implying that a change in the reference database will result in drastically different inferred networks.
  The reason for this seems to derive from the differential labeling of constitutive representative sequences into taxonomic entities (Figure~\ref{fig:figure3} and Figure~\ref{fig:figure_s5}), which drastically alter the nodes and hence the underlying network topology.
  In fact the variability induced by taxonomy assignment is much more significant than that due to the variability introduced based on how the reference sequences themselves are identified (in the DC step).
  The fraction labeled as the residual is an artifact that arises when multiple steps are changed at the same time, however this can be ignored as the fraction is negligible.

  The effects of the different steps of the pipeline on the inferred networks can be best visualized through dimensionality reduction.
  Figure \ref{fig:figure6}B is the PCA plot of all the generated co-occurrence networks, colored by the databases in the TA step (left) and the network inference algorithms in the NI step (right).
  In other words, each point corresponds to a network generated using different combinations of tools, and captures how much the final network is affected by such a choice.
  The importance of the TA step, shown in Figure \ref{fig:figure6}A, is also reflected here since most of the networks can be separated based on the database used (Figure \ref{fig:figure6}B).
  However, it is also evident that some networks, especially those inferred using the direct association based network inference methods, are much closer in the PCA plot regardless of the reference database used.
  Additionally, the variation (on the first two principal axes) between the networks decreases when the low abundance \ac{otu}s are removed from the network (Figure~\ref{fig:figure_s1}).
  These results suggest that the most important criterion for accurate comparative analysis of co-occurrence networks is the taxonomy reference database followed by the level of filtering of the taxonomy tables prior to network inference.

  \FloatBarrier

  \subsection*{The default pipeline}

  The systematic analyses performed in the previous sections illustrate that the choice of tools and parameters can have a big impact on the final co-occurrence network.
  For some of these choices (e.g. for the \ac{op} step), there is no clear metric to establish the best protocol.
  For other choices, the mock communities and synthetic data provide an opportunity to select the combinations of tools that yield more accurate and robust results, though those may depends on assumptions and details of the mock data.
  Despite this partial degree of assessment, we wish to suggest a combination of tools and parameters that led to the best performance based on the mock communities, and displayed the highest robustness to switching to alternative methods.
  These tools and parameters are chosen as the defaults for the pipeline and are given in Table~\ref{tab:micone_tools}.

  Figure~\ref{fig:figure7}A shows the comparison of the default network against networks generated by altering one of the steps of the pipeline from the default.
  This result suggests that the most significant changes (many new nodes and edges are added) occur when the \ac{ta} step, \ac{op} step and the \ac{ni} step are changed.
  However, the L1 distance of networks generated by altering one of the steps of the pipeline from the default against the default network (Figure~\ref{fig:figure7}B) shows that the biggest deviations from the default network occur when the \ac{ni} step is changed.
  These results indicate that the biggest rewiring in networks occur when the network inference algorithm is changed and that the changes in the others steps do not create as significant of a difference.
  % TODO: Show proof??
  The change of taxonomy database and the filtering in the \ac{op} step when turned off also adds a large number of new nodes, but the edges between the common nodes (with the default network) are fairly conserved.

  The low L1 distances show us that not only does the consensus help reconcile the variability in the edges due to multiple network inference algorithms, it also helps reduce the variability due to the other steps of the pipeline as well.
  This result is different from that observed in Figure~\ref{fig:figure6} since in this particular analysis all the steps other than the \ac{ni} step have the consensus algorithm applied to them, which drastically reduces the edges in the graph that have lower probabilities.
  This reinforces the importance of the consensus network in increasing the robustness of the inferred networks to different processing methods.

  Figure~\ref{fig:figure7}C shows the co-occurrence networks inferred for the healthy subjects (control) and autistic subjects in the fecal microbiome transplant study~\cite{Kang2017}.
  This figure demonstrates a typical use case of comparative analysis of networks using the \ac{micone} pipeline.
  These consensus networks were generated using the default tools and parameters from Table~\ref{tab:micone_tools}.
  As a consequence of using the consensus network algorithm, the final co-occurrence networks are sparse and can be visually compared and examined.
  We observe 10 unique links in the network for control samples and 3 unique links in the network for autistic subjects.
  These unique connections can serve as potential starting points for literature surveys and further for experimental confirmations.

