%!TEX root = ../main.tex

\section*{Results}

  \subsection*{\acl{micone} (\acs{micone})}

  We have developed \ac{micone}, a flexible and modular pipeline for 16S amplicon sequencing rRNA data (hereafter mentioned simply as 16S data) analysis, that allows us to infer microbial co-occurrence networks.
  It incorporates various popular, publicly available tools as well as custom Python modules and scripts to facilitate inference of co-occurrence networks from 16S data (see Methods).
  Using \ac{micone} one can obtain co-occurrence networks by applying to 16S data (or to already processed taxonomic count matrices) any combination of the available tools.
  The effects of changing any of the intermediate step can be monitored and evaluated in terms of its final network outcome, as well as on any of the intermediate metrics and data outputs.
  The \ac{micone} pipeline workflow is shown in Figure~\ref{fig:figure1}.
  The different steps for going from 16S data to co-occurrence networks can be grouped into five major modules; (i) the sequence processing (SP) step, which handles the demultiplexing, quality control and trimming of the raw 16S sequencing data into demultiplexed and trimmed sequences; (ii) the denoising and clustering (DC) step, which handles denoising of the demultiplexed and trimmed sequences into representative sequences; (iii) the taxonomy assignment (TA) step that assigns taxonomic labels to the representative sequences; (iv) the \ac{otu} processing (OP) step that filters and transforms the taxonomy abundance table; and finally (v) the network inferences (NI) step which infers the microbial co-occurrence network.
  Each process in the pipeline supports alternate tools for performing the same task (see Methods and Figure~\ref{fig:figure1}).
  A centralized configuration file contains all the specifications for what modules are used in the pipeline, and can be modified by the user to choose the desired set of tools.
  The default tools and parameters for the configuration file can be found in the documentation (see Methods).
  In what follows, we perform a systematic analysis of each step of the pipeline to estimate how much the final co-occurrence network depends on the possible choices at each step.
  We also evaluate a large number of tool combinations to determine a set of recommended default options for the pipeline and provide the users with a set of guidelines to facilitate tool selection as appropriate for their data.

  Our analysis involves two types of data: The first type consists of sets of 16S sequencing data from real communities sampled from human stool microbiomes from a fecal microbiome transplant study of autism~\cite{Kang2017}.
  The second are datasets synthetically or artificially created for the specific goal of helping evaluate computational analysis tools.
  In particular, in order to objectively compare, to the extent possible, how well each step in \ac{micone} best captures the underlying data, we use both mock data (labelled mock4, mock12 and mock16) from mockrobiota~\cite{Bokulich2016} as well as, synthetically generated networks using the NorTA~\cite{Kurtz2015} and seqtime~\cite{Rottjers2018}.
  Detailed information on the mock communities and the settings used to generate the synthetic data are provided in the Methods section.

  \FloatBarrier

  \subsection*{The choice of reference database has the biggest impact on inferred networks}

  In order to analyze the effect of different statistical methods on the inferred co-occurrence networks, we generated co-occurrence networks using all possible combinations of methods and estimated the variability in the networks due to each choice (Figure \ref{fig:figure2}A).
  This analysis and all subsequent analysis, unless explicitly mentioned, are performed on all the samples in the fecal microbiome transplant study.
  The networks are filtered on both association strength (0.1) and pvalue (0.05, only correlation methods) before the networks are subjected to the analyses.
  The effects of various steps on the final co-occurrence network is estimated by building a linear model of the edges of the network as a function the various step in the analysis pipeline (see Methods).
  Figure \ref{fig:figure2}A, shows the percentage of total variation among the co-occurrence networks due to the different steps of the pipeline.
  The TA step, more specifically, the choice of 16S reference database contributes the most ($\sim65.4\%$) to the variation in the networks, followed by the OP step ($\sim26.8\%$).
  This result highlights the importance of the taxonomy assignment step in the 16S data analysis workflow, implying that a change in the reference database will result in drastically different inferred networks.
  The reason for this seems to derive from the differential labeling of constitutive representative sequences into taxonomic entities (Figure~\ref{fig:figure4} and Figure~\ref{fig:figure_s5}), which drastically alter the nodes and hence the underlying network topology.
  In fact the variability induced by taxonomy assignment is much more significant than that due to the variability induced based on how the reference sequences themselves are identified (in the DC step).
  The residual variation (Figure \ref{fig:figure2}A) can be seen as an artifact that arises when multiple steps are changed at the same time, however this is small.

  Figure \ref{fig:figure2}B is the PCA plot of all the generated co-occurrence networks, colored by the databases in the TA step (left) and the network inference algorihtms in the NI step (right).
  In other words, each point corresponds to a network generated using different combination of tools, and captures how much the final network is affected by such choice.
  The importance of the TA step indicated in Figure \ref{fig:figure2}A is also reflected here, in the fact that most of the networks can be clearly separated based on the database used (Figure \ref{fig:figure2}B).
  However, it is also evident that some of the networks, especially those inferred using the direct association based network infernce methods, are much closer in the PCA plot regardless of the reference database used.
  It can also be seen that the variation (on the first two principal axes) between the networks decreases when the low abundance \ac{otu}s are removed from the network (Figure~\ref{fig:figure_s1}).
  These results suggest that the most important criterion for accurate comparative analyses of co-occurrence networks is the taxonomy reference database and the level of filtering of the taxonomy tables prior to network inference.

  \FloatBarrier

  \subsection*{Denoising and clustering methods differ in their identification of less common reference sequences}

  Denoising and clustering are commonly carried out to generate representative sequences from the demultiplexed and trimmed 16S sequencing data to obtain the \ac{otu}/\ac{esv} tables (counts of these representative sequences for each sample).
  In order to compare the representative sequence count tables generated by different tools we processed the same 16S sequencing reads (samples from a fecal microbiome transplant study~\cite{Kang2017}) using 5 different methods: open-reference clustering, closed-reference clustering, denovo clustering, \ac{dada2}~\cite{Callahan2016} and Deblur~\cite{Amir2017}.
  The first three methods are from the vsearch plugin and Deblur is from the deblur plugin of \ac{qiime2} v2021.8.0~\cite{bolyenReproducibleInteractiveScalable2019}.
  The \ac{dada2} method is from the \ac{dada2} R package.

  To compare the representative sequences generated by these methods we employ both the weighted~\cite{Lozupone2007} (Figure~\ref{fig:figure3}A) and unweighted UniFrac method~\cite{Lozupone2005} (Figure~\ref{fig:figure3}B).
  In both the comparisons, we set a count threshold of 10, such that if the count of the representative sequences in a particular sample is less than the threshold, it is omitted from the analysis.
  The values shown in the figures correspond to the mean UniFrac values across all samples in the study.
  The weighted UniFrac distance metric takes into account the counts of the representative sequences, whereas the unweighted UniFrac distance metric does not and hence gives equal weights to each sequence.
  From Figure~\ref{fig:figure3}A one can see that the representative sequences generated by the different methods, with the exception of Deblur, are similar to each other when weighted by their abundance.
  Figure~\ref{fig:figure3}B on the other hand shows an increase in dissimilarity between each pair of methods suggesting that the methods might differ in the assignment of sequences of lower abundance.

  In order to verify this claim, for each of these methods we use only the top 1000 representative sequences and recalculate the distance metrics (Figure \ref{fig:figureS3}).
  We observe that the both the weighted and unweighted UniFrac distances are increased implying that the top representative sequences generated by the different methods are not similar to each other.
  % TODO: Can we make this conclusion?
  This leads us to the conclude that the representative sequences in the middle range of the abundance distribution are those that are most similar between the methods.

  These comparisons only elucidate the pairwise similarity or dissimilarity of a pair of methods.
  In order to determine the tool that most accurately recapitulates the reference sequences in the samples, we used the information about the known 16S sequences from the mock datasets.
  In particular, we used the pipeline to process mock community datasets using each of the possible methods included for this step.
  We next compared the predicted representative sequences with the expected representative sequences and their distribution.
  The results (Figure~\ref{fig:figure3}C and D) show that, for the mock datasets, the predicted sequence distributions are substantially different from the expected sequence distribution.
  This can be seen in the large values for both unweighted and weighted UniFrac distance values when compared to Figure~\ref{fig:figure3}A and B.
  This result is more exaggerated in the case of the unweighted UniFrac metric, where some of the datasets show a very high deviation from the expected sequences.
  These high deviations are primarily in one or two of the three datasets that were analyzed and show that the datasets themselves play a big role in the performance of these methods.
  Open-reference and denovo clustering methods perform the best under the weighted UniFrac metric and the worst (marginally) under the unweighted UniFrac metric.
  This result can be attributed to the large number of low abundance representative sequences that are generated by these methods.
  Deblur performs poorly under weighted Unifrac and although its performance on the mock4 dataset is the best under unweighted UniFrac, its performance on the other datasets is average.
  The Deblur method returns a very small number of representative sequences (2388) and this could account for the reason for the high dissimilarity with the other methods as well as irregular performance on the mock data.
  There is no method that clearly outperforms the rest in all datasets.
  Based on their slightly better performance on the mock datasets, their \textit{de novo} error correcting nature and other previous studies~\cite{Nearing2018}, \ac{dada2} and Deblur seem to be in general the most reliable.
  Overall, since \ac{dada2} as compared to Deblur, displays better performance on all the mock datasets and under the weighted UniFrac metric, we have decided to recommend this tool for the DC step of the pipeline.

  % TODO: Supplementary work - Figure S4
  After the denoising, the sequences are subject to chimera checking.
  The \ac{micone} pipeline supports two different chimera checking methods, uchime and remove bimera.
  The uchime method is from \ac{qiime2}, whereas the remove bimera method is from the \ac{dada2} R package.
  We did not notice any significant difference between the two chimera checking methods, implying that they identify and remove mostly the same set of sequences as chimeras (Figure ???).

  \FloatBarrier

  \subsection*{Taxonomy databases vary widely in taxonomy hierarchy and update frequency}

  Taxonomy databases are used to assign taxonomic identities to the representative sequences obtained after the DC step.
  In order to compare the assigned taxonomies from different databases, we use the same reference sequences and assign taxonomies to them using different taxonomy reference databases.
  The three 16S taxonomic reference databases used in this study are SILVA~\cite{Quast2012}, \ac{gg}~\cite{DeSantis2006} and \ac{ncbi} RefSeq~\cite{Sayers2009}.
  SILVA and \ac{gg} are two popular 16S databases used for taxonomy identification.
  The \ac{ncbi} RefSeq nucleotide database contains 16S rRNA sequences as a part of two BioProjects - 33175 and 33317.
  The three databases vastly differ in terms of their last update status - \ac{gg} was last updated on May 2013, SILVA was last updated on August 2020 at the time of writing and \ac{ncbi} is updated regularly as new sequences are curated.
  Since updates to taxonomic classifications are frequent, these databases vary significantly in terms of taxonomy hierarchies including species names and phylogenetic relationships~\cite{Balvociute2017}.
  Assignment using a particular database also requires a query tool, we use the NaiveBayes classifier from \ac{qiime2} for the \ac{gg} and SILVA databases and the blast tool (also included as a \ac{qiime2} plugin) for the \ac{ncbi} database.
  These tools have been well quantified and optimized~\cite{bokulichOptimizingTaxonomicClassification2018}, hence, we make use of the default parameters in our analyses.

  The representative sequences obtained from the \ac{dada2} method (with remove bimera for chimera checking) in the DC step were used for taxonomic assignment using the three reference databases.
  Figure~\ref{fig:figure4}A depicts a flow diagram (alluvial plot) that shows how the top 50 representative sequences (sorted by abundance) are assigned a Genus according to the three different databases.
  We observe that the top 50 representative sequences are assigned to 17 Genus in \ac{gg}, 23 in SILVA and 17 in \ac{ncbi}, however the sequences that are assigned to a particular Genus vary significantly based on the taxonomy database.
  We observe that not only does the assigned Genus composition vary significantly, but the percentage of unassigned representative sequences (pink) also differ.
  The large percentage of unassigned sequences indicate that even among the most abundant representative sequences a large fraction is assigned an "unknown" Genus in two of the three databases (\ac{ncbi} and \ac{gg}).
  A representative sequence might be assigned an "unknown" Genus for one of two reasons: the first is if the taxonomy identifier associated with the sequence in the database did not contain a Genus; the second (more likely) reason is that the database contains multiple sequences that are very similar to the query (representative) sequence and the consensus algorithm (from \ac{qiime2}) is unable to assign one particular Genus at the required confidence.
  Additionally, we observe that other abundant Genus like Bacteroides and Escherichia are also assigned to different representative sequences (indicated by the different proporitions in the alluvial plot).
  Furthermore, the most abundant Genus (among the top 50 representative sequences) in the \ac{gg} taxonomy table was \textit{Escherichia} whereas it was assigned to "unknown" in NCBI and in the SILVA taxonomy table it was assigned to \textit{Escherichia-Shigella}.
  Although these are minor differences (in the case of \ac{gg} and SILVA), when comparing a large number of taxonomy composition tables these problems are hard to diagnose.

  After assigning all the representative sequences to taxonomies we perform a pairwise comparison of the similarity between the top 100 assignments (by abundance) from different databases at every taxonomic level (Figure~\ref{fig:figure4}B).
  The assignments beyond Family level (Family, Genus and Species) are very dissimilar with $<45\%$ similarity between any pair of databases.
  The matches and mismatches in the assignments from all three databases are comparable, with \ac{ncbi} and SILVA being the most similar across most of the taxonomic levels, followed closed by \ac{gg} vs. SILVA.
  However, the number of mismatches for taxonomy assignments beyond the Genus level (Genus, Species) are very high ($>67\%$), implying that the taxonomy assignments from each reference database are fairly unique and are largely responsible for the differences observed in the co-occurrence networks generated from different taxonomy databases.
  The assignment between SILVA and the other two databases were originally significantly different ($40\%$ mismatch) even at the Phylum level, indicating that 40 of the top 100 assignments at the Phylum level in the SILVA database are different in SILVA.
  However, this was fixed by making minor adjustments to the taxonomic names, such as changing Bacteroidota to Bacteroidetes in the SILVA Phylum assignments.
  The comparison of all assigned genera (Figure~\ref{fig:figure_s5}) instead of the just the top 100 contains a higher percentage of mismatches.
  This suggests that the most abundant sequences are the ones that are consistently matched to the same taxonomies in the different reference databases, at least for the dataset tested in the current analysis.

  As in the previous section, the comparisons so far only indicate similarity or dissimilarity between methods.
  In order to obtain an absolute measure of accuracy of the taxonomic assignments we use the expected reference sequences from the mock datasets as the query sequences for the databases and the expected taxonomic composition as the standard to compare against (Figure~\ref{fig:figure4}C).
  Here, we observe that none of the databases perform better than the others in absolute terms and the dissimilarity is high, indicating that all the databases perform poorly when trying to recapture the expected taxonomic composition.

  Given that no database performs better than others against mock datasets, the choice of which database to use should be driven by other reasons.
  One user-specific way to choose, would be based on the known representation of taxa for the microbiome of interest (see also Discussion). Another reason could be the frequency of updates and the potential for future growth.
  % TODO: Find reference for this
  Since the \ac{gg} database is more widely used in the field~[ref], to enable easier comparison with published networks in literature, we set \ac{gg} as the \ac{micone} standard for taxonomy assignment.
  On the other hand, since the degree of similarity of assignments between \ac{ncbi} and \ac{gg} is fairly high (Figure~\ref{fig:figure_s5}), and due to its regular updation and maintenance the \ac{ncbi} database is also a good choice for the user.

  % TODO: More refs to support our argument?
 % There are also other studies, [ref, OTT paper] that focus on the database aspect and they too have found that...

  \FloatBarrier

  \subsection*{Networks generated using different network inference methods show notable difference in edge-density and connectivity}

  The ten different network inference methods used in this study are: \ac{spieceasi}~\cite{Kurtz2015}, FlashWeave.jl~\cite{tackmannRapidInferenceDirect2019}, \ac{cozine}~\cite{haCompositionalZeroinflatedNetwork2020a}, \ac{harmonies}~\cite{jiangHARMONIESHybridApproach2020}, \ac{spring}~\cite{yoonMicrobialNetworksSPRING2019}, \ac{mldm}~\cite{Yang2017}, \ac{sparcc}~\cite{Friedman2012,Watts2018}, propr~\cite{quinnProprRpackageIdentifying2017}, Spearman and Pearson.
  These network inference methods fall into two groups, the first set of methods (Pearson, Spearman, \ac{sparcc}, propr) infer pairwise correlations while the second set infer direct associations (\ac{spieceasi}, FlashWeave, \ac{cozine}, \ac{harmonies}, \ac{spring}, \ac{mldm}).
  Pairwise correlation methods involve calculation of the correlation coefficient between every pair of node (taxonomic entity like Genus) leading to the detection of spurious indirect connections.
  On the other hand, direct association methods use conditional independence to avoid the detection of correlated but indirectly connected taxonomic entities~\cite{Kurtz2015,Menon2018}.

  For the analysis presented in this section, we used the filtered (filtering during the OP step) Genus level counts table obtained using the \ac{gg} (dada2 and remove bimera methods in the DC step) reference database from the TA step as the input for the network inference algorithms.
  The Genus count tables were obtained by summing up the counts of the lower taxonomy levels (Species and \ac{otu}) that map to the same higher taxonomy level name.
  Figure~\ref{fig:figure5}A shows the networks inferred from this dataset using the different inference algorithms.
  The different networks differ vastly in their edge-density and connectivity; even some of the edges in common to these networks can have their signs inverted.
  Note, however, that some of these comparisons depend on the threshold that has to be applied to the networks (currently 0.3 for the correlation based methods, based on~\cite{Friedman2012} and 0.01 for the direct association methods).
  \ac{mldm} is not shown in the comparisons because the algorithm failed to converge for most of the network combinations, hence, it is only used in the network variation analysis.

  In order to obtain a quantitative picture of the differences between the inferred networks, we analyzed the distribution of common nodes and edges (Figure~\ref{fig:figure5} B and C) using UpSet plots~\cite{Lex}.
  Only \ac{harmonies}, \ac{cozine},\ac{spieceasi}, \ac{spring}, propr, and \ac{sparcc} are used in the comparison since Pearson and Spearman add a large number of spurious edges since they are not intended for compositional datasets).
  The results for the node intersections show that the networks have a large number of nodes in common ($33$ out of $68$ total unique nodes) and no network possesses any unique node.
  The edge intersections in contrast show that only $8$ edges (out of $202$ total unique edges) are in common between all the methods and each network has a large number of unique edges.
  These results indicate that there is a substantial rewiring of connections in the inferred networks.
  Hence, in order to determine the interactions that are most likely to exist in the dataset we advocate for the construction of a consensus algorithm.

  \FloatBarrier

  \subsection*{The consensus network}

  We observe a large number of rewirings in the inferred association networks generated by the different methods (Figure \ref{fig:figure5}C), implying that there is a lack of agreement on the associations that should be a part of the final network.
  Therefore, we have developed a consensus method that would take into consideration the evidence offered by each network inference algorithm and generate a consensus network that would contain the most probable interactions for the input data.
  We have developed two algorithms for the generation of the consensus network, the simple voting and the scaled-sum methods (see Methods).
  We only include \ac{sparcc} and propr in the consensus network calculations, for consensus networks that include Spearman and Pearson see Figure~\ref{fig:figure_s6} in the Supplementary section.
  First, for the correlation-based methods we merge the p-values using the Brown's p-value merging method~\cite{Poole_Gibbs_Shmulevich_Bernard_Knijnenburg_2016,faustCoNetAppInference2016} (see Methods).
  Second, we filter all the inferred networks based on an association strength threshold of 0.1 and a p-value cutoff of 0.05.
  Finally, we apply the consensus algorithms we have developed on these filtered networks.
  The simple voting method does a consensus based on whether the number of inferred networks that have an edge between a pair of nodes is greater than a threshold.
  The scaled-sum method does a consensus based on whether the normalized sum of an edge between a pair of nodes from all inferred networks is greater than a threshold.
  The default algorithm for the pipeline is the scaled-sum method, since this method takes into account the weights of the association in the determination of the final consensus.
  Additionally, by default we include all the network inference methods in the consensus calculation by default, but a subset of methods can be considered if it is required.
  Based on this approach, \ac{micone} reports as default output the consensus network, where is each edge is annotated with weights (correlations for the correlation-based methods and direct associations for the other methods) from all the methods used in the consensus algorithm.

  Similar to the previous steps of the pipeline, where were we evaluated the performance of methods on mock datasets, we can evaluate the network inference algorithms and the final consensus network using synthetic interaction data.
  For this purpose, we generated synthetic interaction data using the ``NorTA''[ref] and ``seqtime''[ref] methods (see Methods), which have been previously used for benchmarking network inference methods.
  For each method, an \ac{otu} counts table is generated based on the selected interaction network topology and properties.
  This counts table is used as the input to the \ac{micone} pipeline to generate predicted associations.
  The interaction network used to generate the counts table is used as the source of true interactions to calculate the precision and sensitivity for each network inference algorithm for the NorTA datasets (Figure \ref{fig:figure6}).
  Each point in the scatter plot represents a network with different input network topology and OTU abundance distributions.
  Overall, we see that the scaled-sum method for a parameter value of 0.5 performs the best in terms of average precision ($0.985$).
  Among the individual inference methods, we observe that \ac{spieceasi} has the best average precision ($0.944$)
  The goal of the consensus network algorithms is to increase the precision of the resultant consensus network by removing edges that are either not present in most of the inference methods or whose association strength is low.
  From Figure \ref{fig:figure6} we can confirm that the consensus algorithm, especially the scaled-sum method, does indeed produce networks with more precision, but as a consequence the sensitivity of the networks is reduced.
  Therefore, the consensus networks provide the means to obtain a short list of associations that would have a high probability of being present in the real interaction network.
  This small set of associations could be further tested and validated experimentally to verify their plausibility.

  % TODO: Supplementary work: Figure S7
  The precision and sensitivity graphs on the seqtime data can be found in the Supplementary section.

  \FloatBarrier

  \subsection*{The default pipeline}

  The systematic analyses performed in the previous sections clearly show that the choice of tools and parameters can have a big impact on the final co-occurrence network.
  For some of these choices (e.g. \ac{dada2} vs. Deblur) there is no clear metric to establish a best protocol.
  For other choices, the mock communities and synthetic data provide an opportunity to select combination of tools that yield more accurate and robust results.
  Despite this partial degree of assessment, we wish to suggest a combination of tools and parameters that produce networks that are derived from the combination of tools which performed best on the mock communities, and displayed highest robustness to switching to alternative methods.
  These tools and parameters are chosen as the defaults for the pipeline and are given in Table~\ref{tab:default_options}.

  The recommended tool for the \ac{dc} step (\ac{dada2}) was chosen based on its accuracy in recapitulating the reference sequences in mock communities.
  For the chimera checking module of the \ac{dc} step, both uchime and remove bimera perform similarly, we suggest using remove bimera since it was developed specifically for \ac{dada2} and should provide the best performance for that denoising method.
  The choice of the taxonomy reference database in the \ac{ta} step is dictated largely by the species expected to be present in the sample as well the database used in similar studies, if comparison is a goal.
  Nevertheless, we suggest \ac{gg} database along with the NaiveBayes classifier as the query tool since the database is used most often in taxonomic studies.
  % TODO: Default OP step values and references
  The abundance threshold at the \ac{op} step is set at ... because large number of nodes are added to networks and decrease the performance of network inference algorithms
  For the \ac{op} step, we recommend that the filtering be turned on and used at the default settings, unless a taxonomy entity of interest might be filtered out at the default settings.
  This is necessary since a large number of rows in the counts matrix would reduce the accuracy of the inference methods and increase the computational time.
  Finally for the \ac{ni}, we recommend the scaled-sum consensus method along with p-value merging (for the correlation-based methods) to generate a final consensus network for the input data.

  Figure~\ref{fig:figure7}A shows the default network compared against networks generated by altering one of the steps of the pipeline from the default.
  These results indicate that the biggest rewiring in networks occur when the reference database or the network inference algorithm are changed.
  % TODO: Show proof??
  The filtering in the \ac{OP} step when turned off also adds a large number of new nodes, but the edges between the common nodes (with the default network) are fairly conserved.
  % TODO: Say that this shows how important the consensus network is in the whole pipeline
  Furthermore, the L1 distance of networks generated by altering one of the steps of the pipeline from the default against the default network (Figure~\ref{fig:figure7}B) shows that the biggest deviations from the default network occur when the \ac{ni} steps are changed.
  This result is different from that observed in Figure~\ref{fig:figure2} since in this particular analyses all the steps other than the \ac{ni} step have the consensus algorithm applied to them, which drastically reduces the edges in the graph that have lower probabilities.
  This result reinforces the importance of the consensus network in increasing the robustness of the inferred networks to different processing methods.
  Figure~\ref{fig:figure7}C shows the co-occurrence networks inferred for the healthy subjects (control) and autistic subjects in the fecal microbiome transplant study~\cite{Kang2017}.
  These consensus networks were generated using the default tools and parameters from Table~\ref{tab:default_options}.
